{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcS0gciJ6VIA"
   },
   "source": [
    "# Quantum Neural Networks (QNNs) for Genomic Pattern Detection\n",
    "**Project By [Adnan Sami Bhuiyan](https://muhammedadnansami.com), and [Hasan Khan](https://osu.github.io/portfolio)**\n",
    "\n",
    "---\n",
    "This project introduces Quantum Neural Networks (QNNs) to analyze genomic data for personalized medicine. With the rise of genetic sequencing, QNNs can detect complex patterns in genetic variants to predict disease risks, drug responses, and optimal treatment paths. By leveraging quantum computation, the project tackles the high-dimensional complexity of genomic pattern recognition, which classical neural networks struggle to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ici5EUr7ZR20",
    "outputId": "0157d62c-9dd0-4b61-e8d8-b8c5712cfb77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in ./idcenv/lib/python3.12/site-packages (0.38.0)\n",
      "Requirement already satisfied: scikit-learn in ./idcenv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: openvino-dev in ./idcenv/lib/python3.12/site-packages (2024.4.0)\n",
      "Requirement already satisfied: pandas in ./idcenv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./idcenv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: onnx in ./idcenv/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: skl2onnx in ./idcenv/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: joblib in ./idcenv/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: onnxruntime in ./idcenv/lib/python3.12/site-packages (1.19.2)\n",
      "Requirement already satisfied: fastatocsv in ./idcenv/lib/python3.12/site-packages (0.0.4)\n",
      "Requirement already satisfied: torch in ./idcenv/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: scipy in ./idcenv/lib/python3.12/site-packages (from pennylane) (1.14.1)\n",
      "Requirement already satisfied: networkx in ./idcenv/lib/python3.12/site-packages (from pennylane) (3.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in ./idcenv/lib/python3.12/site-packages (from pennylane) (0.15.1)\n",
      "Requirement already satisfied: autograd in ./idcenv/lib/python3.12/site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: toml in ./idcenv/lib/python3.12/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in ./idcenv/lib/python3.12/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in ./idcenv/lib/python3.12/site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in ./idcenv/lib/python3.12/site-packages (from pennylane) (5.5.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.38 in ./idcenv/lib/python3.12/site-packages (from pennylane) (0.38.0)\n",
      "Requirement already satisfied: requests in ./idcenv/lib/python3.12/site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in ./idcenv/lib/python3.12/site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in ./idcenv/lib/python3.12/site-packages (from pennylane) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./idcenv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in ./idcenv/lib/python3.12/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in ./idcenv/lib/python3.12/site-packages (from openvino-dev) (2024.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in ./idcenv/lib/python3.12/site-packages (from openvino-dev) (6.0.2)\n",
      "Requirement already satisfied: openvino==2024.4.0 in ./idcenv/lib/python3.12/site-packages (from openvino-dev) (2024.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./idcenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./idcenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./idcenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./idcenv/lib/python3.12/site-packages (from onnx) (3.20.2)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in ./idcenv/lib/python3.12/site-packages (from skl2onnx) (1.14.0)\n",
      "Requirement already satisfied: coloredlogs in ./idcenv/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./idcenv/lib/python3.12/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: sympy in ./idcenv/lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: filelock in ./idcenv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in ./idcenv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./idcenv/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./idcenv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./idcenv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./idcenv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./idcenv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./idcenv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./idcenv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./idcenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./idcenv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./idcenv/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./idcenv/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./idcenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./idcenv/lib/python3.12/site-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./idcenv/lib/python3.12/site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./idcenv/lib/python3.12/site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./idcenv/lib/python3.12/site-packages (from requests->pennylane) (2024.8.30)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./idcenv/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./idcenv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn openvino-dev pandas numpy onnx skl2onnx joblib onnx onnxruntime fastatocsv torch imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IM_C_v6VjkS",
    "outputId": "de04578e-acfd-4a71-a603-02391b5dfeb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SNP data saved to a.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create random SNP data\n",
    "num_snps = 100  # Number of SNPs\n",
    "num_samples = 10  # Number of samples\n",
    "\n",
    "# Randomly generate SNP IDs (e.g., rsIDs)\n",
    "snp_ids = [f\"rs{1000000 + i}\" for i in range(num_snps)]\n",
    "\n",
    "# Randomly assign chromosomes (1-22, X, Y)\n",
    "chromosomes = np.random.choice([f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"], size=num_snps)\n",
    "\n",
    "# Randomly assign positions (1-100,000,000 for demonstration)\n",
    "positions = np.random.randint(1, 100000001, size=num_snps)\n",
    "\n",
    "# Reference and alternative alleles (A, C, G, T)\n",
    "alleles = [\"A\", \"C\", \"G\", \"T\"]\n",
    "reference_alleles = np.random.choice(alleles, size=num_snps)\n",
    "alternative_alleles = np.random.choice(alleles, size=num_snps)\n",
    "\n",
    "# Generate random genotype data for samples (0, 1, 2 representing alleles)\n",
    "genotypes = np.random.randint(0, 3, size=(num_snps, num_samples))\n",
    "\n",
    "# Create sample IDs\n",
    "sample_ids = [f\"Sample_{i+1}\" for i in range(num_samples)]\n",
    "\n",
    "# Create a DataFrame with SNP data\n",
    "df_snp = pd.DataFrame({\n",
    "    \"SNP_ID\": snp_ids,\n",
    "    \"Chromosome\": chromosomes,\n",
    "    \"Position\": positions,\n",
    "    \"Reference_Allele\": reference_alleles,\n",
    "    \"Alternative_Allele\": alternative_alleles\n",
    "})\n",
    "\n",
    "# Add genotype data for each sample\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "    df_snp[sample_id] = genotypes[:, i]\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"a.csv\"\n",
    "df_snp.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Generated SNP data saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbGY5A3OQrNs",
    "outputId": "7d5844b3-2fba-45ad-8df9-bc6d27fbb9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Columns:\n",
      "Index(['SNP_ID', 'Chromosome', 'Position', 'Reference_Allele',\n",
      "       'Alternative_Allele', 'Sample_1', 'Sample_2', 'Sample_3', 'Sample_4',\n",
      "       'Sample_5', 'Sample_6', 'Sample_7', 'Sample_8', 'Sample_9',\n",
      "       'Sample_10'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected numeric SNP data (if any):\n",
      "   Position  Sample_1  Sample_2  Sample_3  Sample_4  Sample_5  Sample_6  \\\n",
      "0  52562568         0         2         2         0         0         1   \n",
      "1  23717336         2         0         0         0         0         1   \n",
      "2  60472383         2         0         0         2         0         2   \n",
      "3  12719243         1         1         2         1         1         2   \n",
      "4  48715251         2         1         1         1         0         0   \n",
      "\n",
      "   Sample_7  Sample_8  Sample_9  Sample_10  \n",
      "0         0         1         0          1  \n",
      "1         0         2         2          0  \n",
      "2         1         0         1          2  \n",
      "3         1         2         1          2  \n",
      "4         0         2         1          2  \n",
      "\n",
      "Missing values in SNP data before cleaning:\n",
      "Position     0\n",
      "Sample_1     0\n",
      "Sample_2     0\n",
      "Sample_3     0\n",
      "Sample_4     0\n",
      "Sample_5     0\n",
      "Sample_6     0\n",
      "Sample_7     0\n",
      "Sample_8     0\n",
      "Sample_9     0\n",
      "Sample_10    0\n",
      "dtype: int64\n",
      "\n",
      "Normalized Data Sample:\n",
      "[[ 0.21260682 -1.14574311  1.31201954  1.33977018 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311 -0.13360105 -1.33977018  0.09747404]\n",
      " [-0.85016274  1.26634765 -1.14035343 -1.11852373 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311  1.08095394  1.11852373 -1.12095141]\n",
      " [ 0.50403491  1.26634765 -1.14035343 -1.11852373  1.29581937 -1.08618493\n",
      "   1.35680105  0.06030227 -1.34815604 -0.11062323  1.31589948]\n",
      " [-1.2553749   0.06030227  0.08583305  1.33977018  0.02540822  0.10742488\n",
      "   1.35680105  0.06030227  1.08095394 -0.11062323  1.31589948]\n",
      " [ 0.07085683  1.26634765  0.08583305  0.11062323  0.02540822 -1.08618493\n",
      "  -1.15579349 -1.14574311  1.08095394 -0.11062323  1.31589948]]\n",
      "\n",
      "Original data shape: (100, 11)\n",
      "Reduced data shape (after PCA): (100, 11)\n",
      "\n",
      "Explained variance of each principal component:\n",
      "[0.12938839 0.12810197 0.11728913 0.10932962 0.09296091 0.08545331\n",
      " 0.07876725 0.0774637  0.06792339 0.06035472 0.05296761]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'a.csv'  # Replace with your actual file path\n",
    "snp_data = pd.read_csv(file_path)\n",
    "\n",
    "# Print the original dataset's structure\n",
    "print(\"Original Dataset Columns:\")\n",
    "print(snp_data.columns)\n",
    "\n",
    "# Step 2: Select numeric SNP columns (assuming SNP data starts after metadata columns)\n",
    "# Replace 'start_column' with the actual index where SNP numeric data starts\n",
    "# Example: If numeric data starts at column 6, adjust the index accordingly\n",
    "\n",
    "# Let's try to infer the numeric columns and select them automatically\n",
    "snp_numeric_data = snp_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debugging: Print if any numeric columns were found\n",
    "print(\"\\nSelected numeric SNP data (if any):\")\n",
    "print(snp_numeric_data.head())\n",
    "\n",
    "# Step 3: Check for missing values in the numeric columns\n",
    "print(\"\\nMissing values in SNP data before cleaning:\")\n",
    "print(snp_numeric_data.isnull().sum())\n",
    "\n",
    "# Step 4: Data Cleaning - Handle missing values (if any)\n",
    "snp_numeric_data.fillna(snp_numeric_data.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Data Normalization - Standardize the SNP data (only if numeric data exists)\n",
    "if not snp_numeric_data.empty:\n",
    "    scaler = StandardScaler()\n",
    "    snp_array_normalized = scaler.fit_transform(snp_numeric_data)\n",
    "\n",
    "    # Print a sample of the normalized data to check scaling\n",
    "    print(\"\\nNormalized Data Sample:\")\n",
    "    print(snp_array_normalized[:5])  # First 5 rows of normalized data\n",
    "\n",
    "    # Step 6: Dimensionality Reduction - Apply PCA\n",
    "    # Changed n_components to be within the valid range (min(n_samples, n_features))\n",
    "    n_components = min(snp_array_normalized.shape[0], snp_array_normalized.shape[1])\n",
    "    pca = PCA(n_components=n_components)  # Reducing to n_components principal components\n",
    "    snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "    # Output the shape of the original and reduced datasets\n",
    "    print(f\"\\nOriginal data shape: {snp_array_normalized.shape}\")\n",
    "    print(f\"Reduced data shape (after PCA): {snp_array_reduced.shape}\")\n",
    "\n",
    "    # If you want to inspect the explained variance for each principal component:\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"\\nExplained variance of each principal component:\")\n",
    "    print(explained_variance)\n",
    "else:\n",
    "    print(\"\\nNo numeric SNP data found. Please verify the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-L-JMZOxZ_Z3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Assuming `snp_array_reduced` is your preprocessed data and `y` are the labels (0 or 1 for disease risk)\n",
    "X = snp_array_reduced\n",
    "y = np.random.randint(0, 2, size=(X.shape[0],))  # Replace with actual labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMYpMrB-aBGL",
    "outputId": "4d327278-ef70-4546-f357-e2ebb9fce1a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.6747\n",
      "Epoch [10/20], Loss: 0.6500\n",
      "Epoch [15/20], Loss: 0.6255\n",
      "Epoch [20/20], Loss: 0.5986\n",
      "Test Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# Define a simple feedforward neural network for disease prediction\n",
    "class SNPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "snp_net_model = SNPNet() # Renamed to avoid name conflict\n",
    "criterion = nn.BCELoss()  # Binary classification\n",
    "optimizer = optim.Adam(snp_net_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    snp_net_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = snp_net_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on test data\n",
    "snp_net_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = snp_net_model(X_test_tensor)\n",
    "    predicted_classes = (predictions >= 0.5).float()  # Binary classification\n",
    "    accuracy = accuracy_score(y_test_tensor, predicted_classes)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AZ-Z9z20aHko"
   },
   "outputs": [],
   "source": [
    "# Export the trained model to ONNX format\n",
    "dummy_input = torch.randn(1, X_train.shape[1])  # Adjust based on input size\n",
    "torch.onnx.export(snp_net_model, dummy_input, \"snp_model.onnx\", opset_version=11) # Use the correct model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQBFHfE7VFxl",
    "outputId": "a34ec073-2cce-45ba-867d-21e5091eb284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted and saved to openvino_model/snp_model.xml\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "import os\n",
    "\n",
    "# Load the ONNX model\n",
    "core = ov.Core()\n",
    "model = core.read_model(\"snp_model.onnx\")\n",
    "\n",
    "# Specify input and output data types\n",
    "input_shape = ov.PartialShape([1, X_train.shape[1]])  # Input shape\n",
    "input_type = ov.Type.f32            # Input data type (FP32)\n",
    "output_type = ov.Type.f32           # Output data type (FP32)\n",
    "\n",
    "# Convert the model to OpenVINO IR with FP32 data type\n",
    "compiled_model = ov.compile_model(model, \"CPU\") # Compile for CPU\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"openvino_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the output file paths\n",
    "xml_path = os.path.join(output_dir, \"snp_model.xml\")\n",
    "bin_path = os.path.join(output_dir, \"snp_model.bin\")\n",
    "\n",
    "# Save the converted model\n",
    "ov.save_model(model, xml_path)  # Save the model\n",
    "print(f\"Model converted and saved to {xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJsxR5fyaRDP",
    "outputId": "5bbb29ec-54d8-42f5-fd72-7fc0696b928e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: {0: 19, 1: 33, 2: 18}\n",
      "Using SMOTE for balancing.\n",
      "Cross-Validation Accuracy: 93.58%\n",
      "Test Accuracy with OpenVINO: 85.00%\n",
      "Patient 1: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 1 raw output: 0.561910\n",
      "Patient 2: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 2 raw output: 0.554574\n",
      "Patient 3: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 3 raw output: 0.584371\n",
      "Patient 4: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 4 raw output: 0.565677\n",
      "Patient 5: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 5 raw output: 0.424276\n",
      "Patient 6: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 6 raw output: 0.466892\n",
      "Patient 7: Predicted Disease Risk - No Disease\n",
      "Patient 7 raw output: 0.279908\n",
      "Patient 8: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 8 raw output: 0.592820\n",
      "Patient 9: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 9 raw output: 0.504283\n",
      "Patient 10: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 10 raw output: 0.610286\n",
      "Patient 11: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 11 raw output: 0.610985\n",
      "Patient 12: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 12 raw output: 0.610282\n",
      "Patient 13: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 13 raw output: 0.572835\n",
      "Patient 14: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 14 raw output: 0.637214\n",
      "Patient 15: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 15 raw output: 0.548164\n",
      "Patient 16: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 16 raw output: 0.635268\n",
      "Patient 17: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 17 raw output: 0.615542\n",
      "Patient 18: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 18 raw output: 0.409417\n",
      "Patient 19: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 19 raw output: 0.456094\n",
      "Patient 20: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 20 raw output: 0.448671\n",
      "Patient 21: Predicted Disease Risk - No Disease\n",
      "Patient 21 raw output: 0.389660\n",
      "Patient 22: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 22 raw output: 0.520499\n",
      "Patient 23: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 23 raw output: 0.495795\n",
      "Patient 24: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 24 raw output: 0.429393\n",
      "Patient 25: Predicted Disease Risk - No Disease\n",
      "Patient 25 raw output: 0.387536\n",
      "Patient 26: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 26 raw output: 0.430796\n",
      "Patient 27: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 27 raw output: 0.457439\n",
      "Patient 28: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 28 raw output: 0.601314\n",
      "Patient 29: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 29 raw output: 0.478035\n",
      "Patient 30: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 30 raw output: 0.562106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "\n",
    "# Assuming SNP data is preprocessed and reduced via PCA\n",
    "# Generate synthetic disease labels for demonstration\n",
    "# 0 = 'No Disease', 1 = 'Heart Disease', 2 = 'Cancer Risk'\n",
    "disease_labels = np.random.choice([0, 1, 2], size=snp_array_reduced.shape[0])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(snp_array_reduced, disease_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check distribution of labels in training data\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Training label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Dynamically decide between SMOTE or RandomOverSampler based on class sizes\n",
    "if min(counts) < 2:\n",
    "    print(\"Using RandomOverSampler due to small class sizes.\")\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_res, y_train_res = oversampler.fit_resample(X_train, y_train)\n",
    "else:\n",
    "    print(\"Using SMOTE for balancing.\")\n",
    "    k_neighbors_value = min(counts) - 1  # To avoid the error, set neighbors based on smallest class size\n",
    "    sm = SMOTE(random_state=42, k_neighbors=k_neighbors_value)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Load the OpenVINO model\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=\"openvino_model/snp_model.xml\")\n",
    "compiled_model = ie.compile_model(model=model_ir, device_name=\"CPU\")  # Use CPU, GPU, etc.\n",
    "\n",
    "# Prepare the input and output layers\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# RandomForest Classifier for better generalization\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%\")\n",
    "\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Run inference on the test data using OpenVINO\n",
    "input_data = X_test  # Ensure test data is in the correct format\n",
    "predictions = []\n",
    "\n",
    "# Iterate over each test data point\n",
    "for i in range(input_data.shape[0]):\n",
    "    single_input = input_data[i].reshape(1, -1)  # Reshape to (1, num_features) for model input\n",
    "    result = compiled_model([single_input])\n",
    "    predictions.append(result[output_layer])\n",
    "\n",
    "# Stack predictions and convert to NumPy array\n",
    "predictions = np.vstack(predictions)\n",
    "\n",
    "# For multiclass, use argmax to get the predicted class index\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map predicted classes to disease names\n",
    "disease_mapping = {0: 'No Disease', 1: 'Heart Disease', 2: 'Cancer Risk'}\n",
    "predicted_diseases = [disease_mapping.get(int(pred), 'Unknown') for pred in predicted_classes]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(f\"Test Accuracy with OpenVINO: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confidence thresholding for better predictions\n",
    "def get_confident_predictions(output_value, confidence_threshold=0.7):\n",
    "    if np.max(output_value) > confidence_threshold:\n",
    "        return np.argmax(output_value)  # Confident prediction\n",
    "    else:\n",
    "        return -1  # Uncertain prediction\n",
    "\n",
    "# Adjust predictions based on confidence levels\n",
    "confident_predictions = [get_confident_predictions(result) for result in predictions]\n",
    "\n",
    "# Define the threshold for disease prediction\n",
    "disease_threshold = 0.6\n",
    "\n",
    "# Function to determine the disease risk based on raw output\n",
    "def get_disease_risk(output_value):\n",
    "    if output_value < 0.4:\n",
    "        return \"No Disease\"\n",
    "    elif 0.4 <= output_value < 0.6:\n",
    "        return \"Possible Heart Disease\"\n",
    "    elif 0.6 <= output_value < 0.7:\n",
    "        return \"Possible Cancer Risk\"\n",
    "    else:\n",
    "        return \"High Risk of Severe Disease\"\n",
    "\n",
    "# Display the predicted disease name for each patient\n",
    "for i, result in enumerate(predictions):\n",
    "    raw_output = result[0]  # Extract raw output\n",
    "    disease_risk = get_disease_risk(raw_output)  # Get disease risk label based on threshold\n",
    "    print(f\"Patient {i+1}: Predicted Disease Risk - {disease_risk}\")\n",
    "    print(f\"Patient {i+1} raw output: {raw_output:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
