{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcS0gciJ6VIA"
   },
   "source": [
    "# Quantum Neural Networks (QNNs) for Genomic Pattern Detection\n",
    "**Project By [Adnan Sami Bhuiyan](https://muhammedadnansami.com)**\n",
    "\n",
    "---\n",
    "This project introduces Quantum Neural Networks (QNNs) to analyze genomic data for personalized medicine. With the rise of genetic sequencing, QNNs can detect complex patterns in genetic variants to predict disease risks, drug responses, and optimal treatment paths. By leveraging quantum computation, the project tackles the high-dimensional complexity of genomic pattern recognition, which classical neural networks struggle to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ici5EUr7ZR20",
    "outputId": "eed623d6-6668-41ac-e48f-1c82a0364a5b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in /opt/anaconda3/lib/python3.12/site-packages (0.38.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: openvino-dev in /opt/anaconda3/lib/python3.12/site-packages (2024.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: onnx in /opt/anaconda3/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: skl2onnx in /opt/anaconda3/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: onnxruntime in /opt/anaconda3/lib/python3.12/site-packages (1.19.2)\n",
      "Requirement already satisfied: fastatocsv in /opt/anaconda3/lib/python3.12/site-packages (0.0.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/lib/python3.12/site-packages (0.12.3)\n",
      "Requirement already satisfied: skorch in /opt/anaconda3/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (3.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (0.15.1)\n",
      "Requirement already satisfied: autograd in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: toml in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (5.3.3)\n",
      "Requirement already satisfied: pennylane-lightning>=0.38 in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (0.38.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (4.11.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from pennylane) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from openvino-dev) (2024.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from openvino-dev) (6.0.1)\n",
      "Requirement already satisfied: openvino==2024.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openvino-dev) (2024.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from onnx) (3.20.2)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from skl2onnx) (1.14.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/anaconda3/lib/python3.12/site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from skorch) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->pennylane) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->pennylane) (2024.8.30)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn openvino-dev pandas numpy onnx skl2onnx joblib onnx onnxruntime fastatocsv torch imbalanced-learn skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IM_C_v6VjkS",
    "outputId": "8ecc34ca-74e3-4d06-cc82-60284d530144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SNP data saved to a.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create random SNP data\n",
    "num_snps = 100  # Number of SNPs\n",
    "num_samples = 10  # Number of samples\n",
    "\n",
    "# Randomly generate SNP IDs (e.g., rsIDs)\n",
    "snp_ids = [f\"rs{1000000 + i}\" for i in range(num_snps)]\n",
    "\n",
    "# Randomly assign chromosomes (1-22, X, Y)\n",
    "chromosomes = np.random.choice([f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"], size=num_snps)\n",
    "\n",
    "# Randomly assign positions (1-100,000,000 for demonstration)\n",
    "positions = np.random.randint(1, 100000001, size=num_snps)\n",
    "\n",
    "# Reference and alternative alleles (A, C, G, T)\n",
    "alleles = [\"A\", \"C\", \"G\", \"T\"]\n",
    "reference_alleles = np.random.choice(alleles, size=num_snps)\n",
    "alternative_alleles = np.random.choice(alleles, size=num_snps)\n",
    "\n",
    "# Generate random genotype data for samples (0, 1, 2 representing alleles)\n",
    "genotypes = np.random.randint(0, 3, size=(num_snps, num_samples))\n",
    "\n",
    "# Create sample IDs\n",
    "sample_ids = [f\"Sample_{i+1}\" for i in range(num_samples)]\n",
    "\n",
    "# Create a DataFrame with SNP data\n",
    "df_snp = pd.DataFrame({\n",
    "    \"SNP_ID\": snp_ids,\n",
    "    \"Chromosome\": chromosomes,\n",
    "    \"Position\": positions,\n",
    "    \"Reference_Allele\": reference_alleles,\n",
    "    \"Alternative_Allele\": alternative_alleles\n",
    "})\n",
    "\n",
    "# Add genotype data for each sample\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "    df_snp[sample_id] = genotypes[:, i]\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"a.csv\"\n",
    "df_snp.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Generated SNP data saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbGY5A3OQrNs",
    "outputId": "bea9c3bf-1bbf-400c-9d11-bfd8915736f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Columns:\n",
      "Index(['SNP_ID', 'Chromosome', 'Position', 'Reference_Allele',\n",
      "       'Alternative_Allele', 'Sample_1', 'Sample_2', 'Sample_3', 'Sample_4',\n",
      "       'Sample_5', 'Sample_6', 'Sample_7', 'Sample_8', 'Sample_9',\n",
      "       'Sample_10'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected numeric SNP data (if any):\n",
      "   Position  Sample_1  Sample_2  Sample_3  Sample_4  Sample_5  Sample_6  \\\n",
      "0  52562568         0         2         2         0         0         1   \n",
      "1  23717336         2         0         0         0         0         1   \n",
      "2  60472383         2         0         0         2         0         2   \n",
      "3  12719243         1         1         2         1         1         2   \n",
      "4  48715251         2         1         1         1         0         0   \n",
      "\n",
      "   Sample_7  Sample_8  Sample_9  Sample_10  \n",
      "0         0         1         0          1  \n",
      "1         0         2         2          0  \n",
      "2         1         0         1          2  \n",
      "3         1         2         1          2  \n",
      "4         0         2         1          2  \n",
      "\n",
      "Missing values in SNP data before cleaning:\n",
      "Position     0\n",
      "Sample_1     0\n",
      "Sample_2     0\n",
      "Sample_3     0\n",
      "Sample_4     0\n",
      "Sample_5     0\n",
      "Sample_6     0\n",
      "Sample_7     0\n",
      "Sample_8     0\n",
      "Sample_9     0\n",
      "Sample_10    0\n",
      "dtype: int64\n",
      "\n",
      "Normalized Data Sample:\n",
      "[[ 0.21260682 -1.14574311  1.31201954  1.33977018 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311 -0.13360105 -1.33977018  0.09747404]\n",
      " [-0.85016274  1.26634765 -1.14035343 -1.11852373 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311  1.08095394  1.11852373 -1.12095141]\n",
      " [ 0.50403491  1.26634765 -1.14035343 -1.11852373  1.29581937 -1.08618493\n",
      "   1.35680105  0.06030227 -1.34815604 -0.11062323  1.31589948]\n",
      " [-1.2553749   0.06030227  0.08583305  1.33977018  0.02540822  0.10742488\n",
      "   1.35680105  0.06030227  1.08095394 -0.11062323  1.31589948]\n",
      " [ 0.07085683  1.26634765  0.08583305  0.11062323  0.02540822 -1.08618493\n",
      "  -1.15579349 -1.14574311  1.08095394 -0.11062323  1.31589948]]\n",
      "\n",
      "Original data shape: (100, 11)\n",
      "\n",
      "Reduced data shape (after PCA): (100, 11)\n",
      "\n",
      "Explained variance of each principal component:\n",
      "[0.12938839 0.12810197 0.11728913 0.10932962 0.09296091 0.08545331\n",
      " 0.07876725 0.0774637  0.06792339 0.06035472 0.05296761]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'a.csv'  # Replace with your actual file path\n",
    "# Use the 'on_bad_lines='skip'' argument to skip rows with incorrect number of fields\n",
    "snp_data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# Print the original dataset's structure\n",
    "print(\"Original Dataset Columns:\")\n",
    "print(snp_data.columns)\n",
    "\n",
    "# Step 2: Select numeric SNP columns (assuming SNP data starts after metadata columns)\n",
    "# Replace 'start_column' with the actual index where SNP numeric data starts\n",
    "# Example: If numeric data starts at column 6, adjust the index accordingly\n",
    "\n",
    "# Let's try to infer the numeric columns and select them automatically\n",
    "snp_numeric_data = snp_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debugging: Print if any numeric columns were found\n",
    "print(\"\\nSelected numeric SNP data (if any):\")\n",
    "print(snp_numeric_data.head())\n",
    "\n",
    "# Step 3: Check for missing values in the numeric columns\n",
    "print(\"\\nMissing values in SNP data before cleaning:\")\n",
    "print(snp_numeric_data.isnull().sum())\n",
    "\n",
    "# Step 4: Data Cleaning - Handle missing values (if any)\n",
    "snp_numeric_data.fillna(snp_numeric_data.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Data Normalization - Standardize the SNP data (only if numeric data exists)\n",
    "if not snp_numeric_data.empty:\n",
    "    scaler = StandardScaler()\n",
    "    snp_array_normalized = scaler.fit_transform(snp_numeric_data)\n",
    "\n",
    "    # Print a sample of the normalized data to check scaling\n",
    "    print(\"\\nNormalized Data Sample:\")\n",
    "    print(snp_array_normalized[:5])  # First 5 rows of normalized data\n",
    "\n",
    "    # Step 6: Dimensionality Reduction - Apply PCA\n",
    "    # Changed n_components to be within the valid range (min(n_samples, n_features))\n",
    "    n_components = min(snp_array_normalized.shape[0], snp_array_normalized.shape[1])\n",
    "    pca = PCA(n_components=n_components)  # Reducing to n_components principal components\n",
    "    snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "    # Output the shape of the original and reduced datasets\n",
    "    print(f\"\\nOriginal data shape: {snp_array_normalized.shape}\")\n",
    "    print(f\"\\nReduced data shape (after PCA): {snp_array_reduced.shape}\")\n",
    "\n",
    "    # If you want to inspect the explained variance for each principal component:\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"\\nExplained variance of each principal component:\")\n",
    "    print(explained_variance)\n",
    "else:\n",
    "    print(\"\\nNo numeric SNP data found. Please verify the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-L-JMZOxZ_Z3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Assuming `snp_array_reduced` is your preprocessed data and `y` are the labels (0 or 1 for disease risk)\n",
    "X = snp_array_reduced\n",
    "y = np.random.randint(0, 2, size=(X.shape[0],))  # Replace with actual labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mIph5vyhn-0z"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.95  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "\n",
    "# Determine the number of components to retain based on explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "optimal_components = (cumulative_variance < explained_variance_threshold).sum() + 1\n",
    "pca = PCA(n_components=optimal_components)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.95  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMYpMrB-aBGL",
    "outputId": "3bdaffeb-56f8-4e18-9e20-eebc4f9c266d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0827\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m0.9822\u001b[0m  0.0116\n",
      "      2        \u001b[36m0.9200\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9290\u001b[0m  0.0018\n",
      "      3        \u001b[36m0.8438\u001b[0m       0.6364        \u001b[35m0.9267\u001b[0m  0.0018\n",
      "      4        \u001b[36m0.7482\u001b[0m       0.6364        0.9447  0.0016\n",
      "      5        \u001b[36m0.7327\u001b[0m       0.5455        0.9455  0.0016\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0725\u001b[0m       \u001b[32m0.3636\u001b[0m        \u001b[35m0.9932\u001b[0m  0.0014\n",
      "      2        \u001b[36m0.9365\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9683\u001b[0m  0.0016\n",
      "      3        \u001b[36m0.8504\u001b[0m       0.5455        0.9842  0.0016\n",
      "      4        \u001b[36m0.7979\u001b[0m       0.4545        1.0310  0.0016\n",
      "      5        \u001b[36m0.7538\u001b[0m       0.4545        1.0348  0.0017\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0729\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m0.9919\u001b[0m  0.0014\n",
      "      2        \u001b[36m0.9361\u001b[0m       0.3636        \u001b[35m0.9814\u001b[0m  0.0017\n",
      "      3        \u001b[36m0.8531\u001b[0m       0.3636        0.9924  0.0019\n",
      "      4        \u001b[36m0.7888\u001b[0m       0.3636        1.0011  0.0017\n",
      "      5        \u001b[36m0.7439\u001b[0m       0.4545        \u001b[35m0.9800\u001b[0m  0.0016\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1194\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0971\u001b[0m  0.0014\n",
      "      2        \u001b[36m1.0943\u001b[0m       0.4545        \u001b[35m1.0799\u001b[0m  0.0015\n",
      "      3        \u001b[36m1.0735\u001b[0m       0.4545        \u001b[35m1.0642\u001b[0m  0.0015\n",
      "      4        \u001b[36m1.0539\u001b[0m       0.4545        \u001b[35m1.0509\u001b[0m  0.0016\n",
      "      5        \u001b[36m1.0347\u001b[0m       0.4545        \u001b[35m1.0378\u001b[0m  0.0015\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1151\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0943\u001b[0m  0.0013\n",
      "      2        \u001b[36m1.0903\u001b[0m       0.4545        \u001b[35m1.0789\u001b[0m  0.0017\n",
      "      3        \u001b[36m1.0699\u001b[0m       0.4545        \u001b[35m1.0644\u001b[0m  0.0019\n",
      "      4        \u001b[36m1.0509\u001b[0m       0.4545        \u001b[35m1.0509\u001b[0m  0.0015\n",
      "      5        \u001b[36m1.0316\u001b[0m       0.4545        \u001b[35m1.0379\u001b[0m  0.0016\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1122\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0925\u001b[0m  0.0013\n",
      "      2        \u001b[36m1.0876\u001b[0m       0.4545        \u001b[35m1.0761\u001b[0m  0.0016\n",
      "      3        \u001b[36m1.0675\u001b[0m       0.4545        \u001b[35m1.0602\u001b[0m  0.0016\n",
      "      4        \u001b[36m1.0483\u001b[0m       0.4545        \u001b[35m1.0456\u001b[0m  0.0016\n",
      "      5        \u001b[36m1.0290\u001b[0m       0.4545        \u001b[35m1.0319\u001b[0m  0.0015\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1131\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0126\u001b[0m  0.0010\n",
      "      2        \u001b[36m0.9814\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9555\u001b[0m  0.0013\n",
      "      3        \u001b[36m0.8935\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9312\u001b[0m  0.0013\n",
      "      4        \u001b[36m0.8359\u001b[0m       0.5455        \u001b[35m0.9281\u001b[0m  0.0013\n",
      "      5        \u001b[36m0.7707\u001b[0m       0.4545        0.9700  0.0012\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1070\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0079\u001b[0m  0.0010\n",
      "      2        \u001b[36m0.9770\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9623\u001b[0m  0.0013\n",
      "      3        \u001b[36m0.8928\u001b[0m       0.7273        \u001b[35m0.9603\u001b[0m  0.0012\n",
      "      4        \u001b[36m0.8446\u001b[0m       0.7273        \u001b[35m0.9595\u001b[0m  0.0014\n",
      "      5        \u001b[36m0.8067\u001b[0m       0.6364        0.9830  0.0012\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1015\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0044\u001b[0m  0.0020\n",
      "      2        \u001b[36m0.9732\u001b[0m       0.4545        \u001b[35m0.9633\u001b[0m  0.0014\n",
      "      3        \u001b[36m0.8957\u001b[0m       0.4545        0.9704  0.0014\n",
      "      4        \u001b[36m0.8319\u001b[0m       0.4545        0.9962  0.0013\n",
      "      5        \u001b[36m0.7910\u001b[0m       0.3636        0.9821  0.0013\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1237\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.1021\u001b[0m  0.0010\n",
      "      2        \u001b[36m1.1048\u001b[0m       0.4545        \u001b[35m1.0893\u001b[0m  0.0015\n",
      "      3        \u001b[36m1.0897\u001b[0m       0.4545        \u001b[35m1.0779\u001b[0m  0.0015\n",
      "      4        \u001b[36m1.0757\u001b[0m       0.4545        \u001b[35m1.0673\u001b[0m  0.0014\n",
      "      5        \u001b[36m1.0621\u001b[0m       0.4545        \u001b[35m1.0575\u001b[0m  0.0013\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1195\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0993\u001b[0m  0.0010\n",
      "      2        \u001b[36m1.1009\u001b[0m       0.4545        \u001b[35m1.0876\u001b[0m  0.0012\n",
      "      3        \u001b[36m1.0855\u001b[0m       0.4545        \u001b[35m1.0771\u001b[0m  0.0013\n",
      "      4        \u001b[36m1.0717\u001b[0m       0.4545        \u001b[35m1.0670\u001b[0m  0.0012\n",
      "      5        \u001b[36m1.0587\u001b[0m       0.4545        \u001b[35m1.0573\u001b[0m  0.0017\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1163\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0974\u001b[0m  0.0011\n",
      "      2        \u001b[36m1.0973\u001b[0m       0.4545        \u001b[35m1.0845\u001b[0m  0.0014\n",
      "      3        \u001b[36m1.0815\u001b[0m       0.4545        \u001b[35m1.0730\u001b[0m  0.0015\n",
      "      4        \u001b[36m1.0672\u001b[0m       0.4545        \u001b[35m1.0624\u001b[0m  0.0019\n",
      "      5        \u001b[36m1.0535\u001b[0m       0.4545        \u001b[35m1.0521\u001b[0m  0.0014\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0527\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m0.9752\u001b[0m  0.0018\n",
      "      2        \u001b[36m0.9176\u001b[0m       \u001b[32m0.5000\u001b[0m        0.9935  0.0124\n",
      "      3        \u001b[36m0.8269\u001b[0m       0.4375        1.0434  0.0026\n",
      "      4        \u001b[36m0.7708\u001b[0m       0.4375        1.0938  0.0079\n",
      "      5        \u001b[36m0.7361\u001b[0m       0.3750        1.0949  0.0028\n",
      "Best hyperparameters found: {'batch_size': 16, 'lr': 0.01}\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0488\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m0.9808\u001b[0m  0.0018\n",
      "      2        \u001b[36m0.9099\u001b[0m       \u001b[32m0.5625\u001b[0m        0.9886  0.0026\n",
      "      3        \u001b[36m0.8953\u001b[0m       0.3750        1.0883  0.0023\n",
      "      4        \u001b[36m0.7849\u001b[0m       0.4375        1.0308  0.0028\n",
      "      5        \u001b[36m0.7747\u001b[0m       0.4375        1.0495  0.0021\n",
      "      6        \u001b[36m0.7383\u001b[0m       0.3750        1.1409  0.0028\n",
      "      7        \u001b[36m0.7051\u001b[0m       0.3125        1.1696  0.0027\n",
      "      8        \u001b[36m0.6752\u001b[0m       0.3750        1.1711  0.0021\n",
      "      9        \u001b[36m0.6440\u001b[0m       0.4375        1.1509  0.0021\n",
      "     10        \u001b[36m0.6038\u001b[0m       0.3125        1.1895  0.0023\n",
      "     11        \u001b[36m0.6032\u001b[0m       0.2500        1.2100  0.0021\n",
      "     12        \u001b[36m0.5914\u001b[0m       0.3750        1.1705  0.0021\n",
      "     13        \u001b[36m0.5867\u001b[0m       0.3750        1.1474  0.0022\n",
      "     14        \u001b[36m0.5860\u001b[0m       0.3750        1.1499  0.0022\n",
      "     15        \u001b[36m0.5845\u001b[0m       0.3750        1.1786  0.0023\n",
      "     16        \u001b[36m0.5836\u001b[0m       0.3750        1.1991  0.0021\n",
      "     17        \u001b[36m0.5835\u001b[0m       0.3125        1.2081  0.0021\n",
      "     18        \u001b[36m0.5834\u001b[0m       0.3125        1.2096  0.0022\n",
      "     19        \u001b[36m0.5833\u001b[0m       0.3125        1.2010  0.0023\n",
      "     20        \u001b[36m0.5831\u001b[0m       0.3125        1.1915  0.0021\n",
      "     21        \u001b[36m0.5831\u001b[0m       0.3750        1.1846  0.0024\n",
      "     22        \u001b[36m0.5830\u001b[0m       0.3750        1.1798  0.0021\n",
      "     23        \u001b[36m0.5830\u001b[0m       0.3750        1.1766  0.0022\n",
      "     24        \u001b[36m0.5829\u001b[0m       0.3750        1.1749  0.0022\n",
      "     25        \u001b[36m0.5829\u001b[0m       0.3750        1.1747  0.0021\n",
      "     26        \u001b[36m0.5829\u001b[0m       0.3750        1.1743  0.0020\n",
      "     27        \u001b[36m0.5829\u001b[0m       0.3750        1.1737  0.0024\n",
      "     28        \u001b[36m0.5828\u001b[0m       0.3750        1.1729  0.0020\n",
      "     29        \u001b[36m0.5828\u001b[0m       0.3750        1.1718  0.0021\n",
      "     30        \u001b[36m0.5828\u001b[0m       0.3750        1.1713  0.0022\n",
      "     31        \u001b[36m0.5828\u001b[0m       0.3750        1.1708  0.0021\n",
      "     32        \u001b[36m0.5828\u001b[0m       0.3750        1.1700  0.0020\n",
      "     33        \u001b[36m0.5828\u001b[0m       0.3750        1.1696  0.0022\n",
      "     34        \u001b[36m0.5828\u001b[0m       0.3750        1.1691  0.0019\n",
      "     35        \u001b[36m0.5828\u001b[0m       0.3750        1.1689  0.0020\n",
      "     36        \u001b[36m0.5828\u001b[0m       0.3750        1.1685  0.0022\n",
      "     37        \u001b[36m0.5828\u001b[0m       0.3750        1.1684  0.0021\n",
      "     38        \u001b[36m0.5828\u001b[0m       0.3750        1.1679  0.0020\n",
      "     39        \u001b[36m0.5828\u001b[0m       0.3750        1.1677  0.0022\n",
      "     40        \u001b[36m0.5828\u001b[0m       0.3750        1.1672  0.0021\n",
      "     41        \u001b[36m0.5828\u001b[0m       0.3750        1.1670  0.0020\n",
      "     42        \u001b[36m0.5828\u001b[0m       0.3750        1.1672  0.0021\n",
      "     43        \u001b[36m0.5828\u001b[0m       0.3750        1.1670  0.0020\n",
      "     44        \u001b[36m0.5828\u001b[0m       0.3750        1.1671  0.0021\n",
      "     45        \u001b[36m0.5828\u001b[0m       0.3750        1.1668  0.0029\n",
      "     46        \u001b[36m0.5828\u001b[0m       0.3750        1.1667  0.0023\n",
      "     47        \u001b[36m0.5828\u001b[0m       0.3750        1.1666  0.0021\n",
      "     48        \u001b[36m0.5828\u001b[0m       0.3750        1.1664  0.0023\n",
      "     49        \u001b[36m0.5828\u001b[0m       0.3750        1.1662  0.0021\n",
      "     50        \u001b[36m0.5828\u001b[0m       0.3750        1.1661  0.0023\n",
      "Test Accuracy: 65.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1n/c1yjscj17_d3hw1tj4cjj2p80000gn/T/ipykernel_76735/2814670394.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# Define the deep learning model for multi-class classification\n",
    "class DeepSNPNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeepSNPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # 3 output classes (No Disease, Heart Disease, Cancer Risk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)  # Softmax for multi-class classification\n",
    "        return x\n",
    "\n",
    "# Set input size based on your data\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Initialize model for multi-class classification\n",
    "model = DeepSNPNet(input_size)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.001],  # Start with fewer values to confirm functionality\n",
    "    'batch_size': [16, 32]  # Smaller batch sizes to speed up training\n",
    "}\n",
    "\n",
    "# Wrap the model with skorch's NeuralNetClassifier for multi-class classification\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=5,  # Start with a smaller epoch count for testing\n",
    "    lr=0.01,  # Default learning rate (grid search will test variations)\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,  # Use CrossEntropyLoss for multi-class\n",
    "    iterator_train__shuffle=True,\n",
    "    device='xpu' if torch.xpu.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Convert X_train to float32 before fitting\n",
    "X_train_float32 = X_train.astype(np.float32)\n",
    "\n",
    "# Convert training data into a DataLoader for better performance\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_float32), torch.tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(net, param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train_float32, y_train)\n",
    "\n",
    "# Retrieve and print the best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Apply best parameters to the model for final training\n",
    "optimized_net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=50,  # Set to the desired epoch count after testing\n",
    "    lr=best_params['lr'],\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    device='xpu' if torch.xpu.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Convert test set to float32 for compatibility\n",
    "X_test_float32 = X_test.astype(np.float32)\n",
    "\n",
    "# Initialize scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train with the optimized model and parameters\n",
    "optimized_net.fit(X_train_float32, y_train)\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = optimized_net.predict(X_test_float32)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AZ-Z9z20aHko"
   },
   "outputs": [],
   "source": [
    "device = next(optimized_net.module_.parameters()).device  # Get model's device\n",
    "dummy_input = torch.randn(1, X_train.shape[1], device=device)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(optimized_net.module_, dummy_input, \"snp_model.onnx\", opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQBFHfE7VFxl",
    "outputId": "5b26d360-5842-4e95-c3df-9a9840490ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted and saved to openvino_model/snp_model.xml\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "import os\n",
    "\n",
    "# Load the ONNX model\n",
    "core = ov.Core()\n",
    "model = core.read_model(\"snp_model.onnx\")\n",
    "\n",
    "# Specify input and output data types\n",
    "input_shape = ov.PartialShape([1, X_train.shape[1]])  # Input shape\n",
    "input_type = ov.Type.f32            # Input data type (FP32)\n",
    "output_type = ov.Type.f32           # Output data type (FP32)\n",
    "\n",
    "# Convert the model to OpenVINO IR with FP32 data type\n",
    "compiled_model = ov.compile_model(model, \"CPU\") # Compile for CPU\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"openvino_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the output file paths\n",
    "xml_path = os.path.join(output_dir, \"snp_model.xml\")\n",
    "bin_path = os.path.join(output_dir, \"snp_model.bin\")\n",
    "\n",
    "# Save the converted model\n",
    "ov.save_model(model, xml_path)  # Save the model\n",
    "print(f\"Model converted and saved to {xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJsxR5fyaRDP",
    "outputId": "c8bef613-942a-48f6-9f42-81595f654cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: {0: 26, 1: 23, 2: 21}\n",
      "Using SMOTE for balancing.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/cpp/core.cpp:107:\nException from src/inference/src/dev/core_impl.cpp:566:\nDevice with \"GPU\" name is not registered in the OpenVINO Runtime\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m ie \u001b[38;5;241m=\u001b[39m Core()\n\u001b[1;32m     43\u001b[0m model_ir \u001b[38;5;241m=\u001b[39m ie\u001b[38;5;241m.\u001b[39mread_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvino_model/snp_model.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m compiled_model \u001b[38;5;241m=\u001b[39m ie\u001b[38;5;241m.\u001b[39mcompile_model(model\u001b[38;5;241m=\u001b[39mmodel_ir, device_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use CPU, GPU, etc.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Get input shape from OpenVINO model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m compiled_model\u001b[38;5;241m.\u001b[39minput(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# Get the expected input shape\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openvino/runtime/ie_api.py:543\u001b[0m, in \u001b[0;36mCore.compile_model\u001b[0;34m(self, model, device_name, config, weights)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m CompiledModel(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile_model(model, {} \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m config),\n\u001b[1;32m    541\u001b[0m         )\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledModel(\n\u001b[0;32m--> 543\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile_model(model, device_name, {} \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m config),\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/core.cpp:107:\nException from src/inference/src/dev/core_impl.cpp:566:\nDevice with \"GPU\" name is not registered in the OpenVINO Runtime\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Assuming SNP data is preprocessed and reduced via PCA\n",
    "# Generate synthetic disease labels for demonstration\n",
    "# 0 = 'No Disease', 1 = 'Heart Disease', 2 = 'Cancer Risk'\n",
    "disease_labels = np.random.choice([0, 1, 2], size=snp_array_reduced.shape[0])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(snp_array_reduced, disease_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check distribution of labels in training data\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Training label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Dynamically decide between SMOTE or RandomOverSampler based on class sizes\n",
    "if min(counts) < 2:\n",
    "    print(\"Using RandomOverSampler due to small class sizes.\")\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_res, y_train_res = oversampler.fit_resample(X_train, y_train)\n",
    "else:\n",
    "    print(\"Using SMOTE for balancing.\")\n",
    "    k_neighbors_value = min(counts) - 1  # To avoid the error, set neighbors based on smallest class size\n",
    "    sm = SMOTE(random_state=42, k_neighbors=k_neighbors_value)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Load the OpenVINO model\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=\"openvino_model/snp_model.xml\")\n",
    "compiled_model = ie.compile_model(model=model_ir, device_name=\"GPU\")  # Use CPU, GPU, etc.\n",
    "\n",
    "# Get input shape from OpenVINO model\n",
    "input_shape = compiled_model.input(0).shape  # Get the expected input shape\n",
    "num_features_openvino = input_shape[1]  # Number of features expected by OpenVINO\n",
    "\n",
    "# Prepare the input and output layers\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# RandomForest Classifier for better generalization\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    # ... other parameters (add more if needed)\n",
    "}\n",
    "# Create the VotingClassifier (ensemble_model)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(ensemble_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%\")\n",
    "\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%\")\n",
    "\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Run inference on the test data using OpenVINO and deep learning model\n",
    "input_data = X_test  # Ensure test data is in the correct format\n",
    "predictions_openvino = []\n",
    "predictions_deep_learning = []\n",
    "\n",
    "# Get the input size for the deep learning model\n",
    "deep_learning_input_size = optimized_net.module_.fc1.in_features  # Get input size from fc1 layer\n",
    "\n",
    "# Iterate over each test data point\n",
    "for i in range(X_test.shape[0]): # Use X_test directly for iteration\n",
    "    single_input = X_test[i].reshape(1, -1)  # Reshape to (1, num_features)\n",
    "\n",
    "    # Reshape input to match OpenVINO model's expected shape if necessary\n",
    "    if single_input.shape[1] != num_features_openvino:\n",
    "        # Pad or truncate the input to match the expected shape\n",
    "        single_input = np.pad(single_input, ((0, 0), (0, num_features_openvino - single_input.shape[1])), 'constant')\n",
    "\n",
    "    # OpenVINO inference\n",
    "    result_openvino = compiled_model([single_input])\n",
    "    predictions_openvino.append(result_openvino[output_layer])\n",
    "\n",
    "    # Reshape or pad/truncate the input to match deep learning model's input size\n",
    "    if single_input.shape[1] != deep_learning_input_size:\n",
    "        single_input_dl = np.pad(single_input, ((0, 0), (0, deep_learning_input_size - single_input.shape[1])), 'constant')\n",
    "    else:\n",
    "        single_input_dl = single_input\n",
    "\n",
    "    # Deep learning model inference\n",
    "    result_deep_learning = optimized_net.module_(torch.tensor(single_input_dl, dtype=torch.float32, device=device))  # Use optimized_net.module_\n",
    "    predictions_deep_learning.append(result_deep_learning.cpu().detach().numpy())\n",
    "\n",
    "# Stack predictions and convert to NumPy arrays\n",
    "predictions_openvino = np.vstack(predictions_openvino)\n",
    "predictions_deep_learning = np.vstack(predictions_deep_learning)\n",
    "\n",
    "# Combine predictions (e.g., averaging or weighted averaging)\n",
    "combined_predictions = (predictions_openvino + predictions_deep_learning) / 2  # Simple averaging\n",
    "\n",
    "# For multiclass, use argmax to get the predicted class index\n",
    "predicted_classes = np.argmax(combined_predictions, axis=1)\n",
    "\n",
    "# Map predicted classes to disease names\n",
    "disease_mapping = {0: 'No Disease', 1: 'Heart Disease', 2: 'Cancer Risk'}\n",
    "predicted_diseases = [disease_mapping.get(int(pred), 'Unknown') for pred in predicted_classes]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(f\"Test Accuracy with OpenVINO: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confidence thresholding for better predictions\n",
    "def get_confident_predictions(output_value, confidence_threshold=0.7):\n",
    "    if np.max(output_value) > confidence_threshold:\n",
    "        return np.argmax(output_value)  # Confident prediction\n",
    "    else:\n",
    "        return -1  # Uncertain prediction\n",
    "\n",
    "# Adjust predictions based on confidence levels\n",
    "confident_predictions = [get_confident_predictions(result) for result in predictions]\n",
    "# Define the threshold for disease prediction\n",
    "disease_threshold = 0.6\n",
    "\n",
    "# Function to determine the disease risk based on raw output\n",
    "# This function needs to be adjusted to handle the output of the ensemble model\n",
    "def get_disease_risk(predicted_class):  # Changed input to predicted_class\n",
    "    # Map the predicted class to a risk level\n",
    "    if predicted_class == 0:\n",
    "        return \"No Disease\"\n",
    "    elif predicted_class == 1:\n",
    "        return \"Possible Heart Disease\"\n",
    "    elif predicted_class == 2:\n",
    "        return \"Possible Cancer Risk\"\n",
    "    else:\n",
    "        return \"Unknown\" # Handle unexpected class values\n",
    "\n",
    "\n",
    "# Accessing prediction probabilities for a more nuanced approach\n",
    "# This approach assumes the voting classifier can provide probabilities\n",
    "# Make sure you use voting='soft' in your VotingClassifier for this\n",
    "predictions_with_probs = ensemble_model.predict_proba(X_test)\n",
    "\n",
    "# Display the predicted disease name and probabilities for each patient\n",
    "for i, result in enumerate(predictions_with_probs):\n",
    "    # Get predicted class directly from result (using argmax for probabilities)\n",
    "    predicted_class = np.argmax(result)  # Get predicted class from probabilities\n",
    "    disease_risk = get_disease_risk(predicted_class)\n",
    "    print(f\"Patient {i+1}: Predicted Disease Risk - {disease_risk}\")\n",
    "    print(f\"Patient {i+1} probabilities: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGrL3b91LrdA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
