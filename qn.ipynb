{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcS0gciJ6VIA"
   },
   "source": [
    "# Quantum Neural Networks (QNNs) for Genomic Pattern Detection\n",
    "**Project By [Adnan Sami Bhuiyan](https://muhammedadnansami.com), and [Hasan Khan](https://osu.github.io/portfolio), Mehedi Hassan Maruf**\n",
    "\n",
    "---\n",
    "This project introduces Quantum Neural Networks (QNNs) to analyze genomic data for personalized medicine. With the rise of genetic sequencing, QNNs can detect complex patterns in genetic variants to predict disease risks, drug responses, and optimal treatment paths. By leveraging quantum computation, the project tackles the high-dimensional complexity of genomic pattern recognition, which classical neural networks struggle to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ici5EUr7ZR20",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "eed623d6-6668-41ac-e48f-1c82a0364a5b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in ./lib/python3.12/site-packages (0.38.0)\n",
      "Requirement already satisfied: scikit-learn in ./lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: openvino-dev in ./lib/python3.12/site-packages (2024.4.0)\n",
      "Requirement already satisfied: pandas in ./lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: onnx in ./lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: skl2onnx in ./lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: joblib in ./lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: onnxruntime in ./lib/python3.12/site-packages (1.19.2)\n",
      "Requirement already satisfied: fastatocsv in ./lib/python3.12/site-packages (0.0.4)\n",
      "Requirement already satisfied: torch in ./lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in ./lib/python3.12/site-packages (0.12.4)\n",
      "Requirement already satisfied: skorch in ./lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in ./lib/python3.12/site-packages (from pennylane) (1.14.1)\n",
      "Requirement already satisfied: networkx in ./lib/python3.12/site-packages (from pennylane) (3.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in ./lib/python3.12/site-packages (from pennylane) (0.15.1)\n",
      "Requirement already satisfied: autograd in ./lib/python3.12/site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: toml in ./lib/python3.12/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in ./lib/python3.12/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in ./lib/python3.12/site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in ./lib/python3.12/site-packages (from pennylane) (5.5.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.38 in ./lib/python3.12/site-packages (from pennylane) (0.38.0)\n",
      "Requirement already satisfied: requests in ./lib/python3.12/site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.12/site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in ./lib/python3.12/site-packages (from pennylane) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in ./lib/python3.12/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in ./lib/python3.12/site-packages (from openvino-dev) (2024.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in ./lib/python3.12/site-packages (from openvino-dev) (6.0.2)\n",
      "Requirement already satisfied: openvino==2024.4.0 in ./lib/python3.12/site-packages (from openvino-dev) (2024.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./lib/python3.12/site-packages (from onnx) (3.20.2)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in ./lib/python3.12/site-packages (from skl2onnx) (1.14.0)\n",
      "Requirement already satisfied: coloredlogs in ./lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./lib/python3.12/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: sympy in ./lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: filelock in ./lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in ./lib/python3.12/site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in ./lib/python3.12/site-packages (from skorch) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.12/site-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.12/site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.12/site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.12/site-packages (from requests->pennylane) (2024.8.30)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn openvino-dev pandas numpy onnx skl2onnx joblib onnx onnxruntime fastatocsv torch imbalanced-learn skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbGY5A3OQrNs",
    "outputId": "bea9c3bf-1bbf-400c-9d11-bfd8915736f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Columns:\n",
      "Index(['ID', 'Pedigree', 'DYSKIN', 'EASPIN', 'EPPIN', 'PLHTIN', 'PLHTUN',\n",
      "       'PLSTIN', 'STRCO1', 'STRCO2', 'STRRAT1', 'STRRAT2', 'YLDIN'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected numeric SNP data (if any):\n",
      "   ID  DYSKIN  EASPIN    EPPIN   PLHTIN   PLHTUN  PLSTIN  STRCO1  STRCO2  \\\n",
      "0   1  64.875   2.625  0.80700  136.875  142.375  15.250  26.875  32.000   \n",
      "1   2  63.500   2.938  0.97553  131.500  131.000  15.750  10.625  25.250   \n",
      "2   3  63.125   2.125  1.07938  117.250  108.875  16.375   4.250   7.250   \n",
      "3   4  66.375   3.000  0.88410  124.375  141.375  15.625   4.625   8.375   \n",
      "4   5  65.125   2.500  0.94498  141.750  148.125  14.875   3.000   5.500   \n",
      "\n",
      "   STRRAT1  STRRAT2    YLDIN  \n",
      "0    4.750    6.875  2497.76  \n",
      "1    3.875    5.625  1872.19  \n",
      "2    2.875    3.875  3238.67  \n",
      "3    3.750    5.250  1796.90  \n",
      "4    2.875    4.000  2508.93  \n",
      "\n",
      "Missing values in SNP data before cleaning:\n",
      "ID         0\n",
      "DYSKIN     0\n",
      "EASPIN     0\n",
      "EPPIN      0\n",
      "PLHTIN     0\n",
      "PLHTUN     0\n",
      "PLSTIN     0\n",
      "STRCO1     0\n",
      "STRCO2     0\n",
      "STRRAT1    0\n",
      "STRRAT2    0\n",
      "YLDIN      0\n",
      "dtype: int64\n",
      "\n",
      "Normalized Data Sample:\n",
      "[[-1.72054204e+00  2.04983060e-01 -9.02276056e-01  1.70476136e-01\n",
      "   8.77692679e-01  5.42962201e-01 -2.78530621e-01  2.37261357e-01\n",
      "  -1.49744529e-01  6.95515650e-01  9.12597427e-01  1.06984673e+00]\n",
      " [-1.69744751e+00 -4.38691814e-02 -1.50752837e-01  1.08030148e+00\n",
      "   4.88545234e-01 -2.66844380e-01  2.31997669e-01 -8.45297435e-01\n",
      "  -5.00426587e-01 -1.69610955e-01 -2.71536006e-01  1.15822962e-01]\n",
      " [-1.67435299e+00 -1.11737975e-01 -2.10279238e+00  1.64094564e+00\n",
      "  -5.43147991e-01 -1.84196267e+00  8.70158031e-01 -1.26999358e+00\n",
      "  -1.43557874e+00 -1.15832707e+00 -1.92932281e+00  2.19976944e+00]\n",
      " [-1.65125846e+00  4.76458233e-01 -1.88881235e-03  5.86707868e-01\n",
      "  -2.73013781e-02  4.71770414e-01  1.04365596e-01 -1.24501145e+00\n",
      "  -1.37713173e+00 -2.93200470e-01 -6.26776035e-01  1.00215934e-03]\n",
      " [-1.62816394e+00  2.50228922e-01 -1.20240514e+00  9.15374376e-01\n",
      "   1.23064036e+00  9.52314978e-01 -6.61426838e-01 -1.35326733e+00\n",
      "  -1.52649631e+00 -1.15832707e+00 -1.81090947e+00  1.08688151e+00]]\n",
      "\n",
      "Original data shape: (150, 12)\n",
      "\n",
      "Reduced data shape (after PCA): (150, 12)\n",
      "\n",
      "Explained variance of each principal component:\n",
      "[0.41313576 0.16251035 0.13197431 0.08841108 0.06782081 0.05543116\n",
      " 0.03722716 0.01803495 0.01061467 0.00701337 0.00480681 0.00301959]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'a.csv'  # Replace with your actual file path\n",
    "# Use the 'on_bad_lines='skip'' argument to skip rows with incorrect number of fields\n",
    "snp_data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# Print the original dataset's structure\n",
    "print(\"Original Dataset Columns:\")\n",
    "print(snp_data.columns)\n",
    "\n",
    "# Step 2: Select numeric SNP columns (assuming SNP data starts after metadata columns)\n",
    "# Replace 'start_column' with the actual index where SNP numeric data starts\n",
    "# Example: If numeric data starts at column 6, adjust the index accordingly\n",
    "\n",
    "# Let's try to infer the numeric columns and select them automatically\n",
    "snp_numeric_data = snp_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debugging: Print if any numeric columns were found\n",
    "print(\"\\nSelected numeric SNP data (if any):\")\n",
    "print(snp_numeric_data.head())\n",
    "\n",
    "# Step 3: Check for missing values in the numeric columns\n",
    "print(\"\\nMissing values in SNP data before cleaning:\")\n",
    "print(snp_numeric_data.isnull().sum())\n",
    "\n",
    "# Step 4: Data Cleaning - Handle missing values (if any)\n",
    "snp_numeric_data.fillna(snp_numeric_data.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Data Normalization - Standardize the SNP data (only if numeric data exists)\n",
    "if not snp_numeric_data.empty:\n",
    "    scaler = StandardScaler()\n",
    "    snp_array_normalized = scaler.fit_transform(snp_numeric_data)\n",
    "\n",
    "    # Print a sample of the normalized data to check scaling\n",
    "    print(\"\\nNormalized Data Sample:\")\n",
    "    print(snp_array_normalized[:5])  # First 5 rows of normalized data\n",
    "\n",
    "    # Step 6: Dimensionality Reduction - Apply PCA\n",
    "    # Changed n_components to be within the valid range (min(n_samples, n_features))\n",
    "    n_components = min(snp_array_normalized.shape[0], snp_array_normalized.shape[1])\n",
    "    pca = PCA(n_components=n_components)  # Reducing to n_components principal components\n",
    "    snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "    # Output the shape of the original and reduced datasets\n",
    "    print(f\"\\nOriginal data shape: {snp_array_normalized.shape}\")\n",
    "    print(f\"\\nReduced data shape (after PCA): {snp_array_reduced.shape}\")\n",
    "\n",
    "    # If you want to inspect the explained variance for each principal component:\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"\\nExplained variance of each principal component:\")\n",
    "    print(explained_variance)\n",
    "else:\n",
    "    print(\"\\nNo numeric SNP data found. Please verify the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "-L-JMZOxZ_Z3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Assuming `snp_array_reduced` is your preprocessed data and `y` are the labels (0 or 1 for disease risk)\n",
    "X = snp_array_reduced\n",
    "y = np.random.randint(0, 2, size=(X.shape[0],))  # Replace with actual labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "mIph5vyhn-0z"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.99  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "\n",
    "# Determine the number of components to retain based on explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "optimal_components = (cumulative_variance < explained_variance_threshold).sum() + 1\n",
    "pca = PCA(n_components=optimal_components)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.95  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMYpMrB-aBGL",
    "outputId": "3bdaffeb-56f8-4e18-9e20-eebc4f9c266d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0505\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.9543\u001b[0m  0.0082\n",
      "      2        \u001b[36m0.9320\u001b[0m       0.6250        0.9568  0.0048\n",
      "      3        \u001b[36m0.8901\u001b[0m       0.4375        0.9770  0.0039\n",
      "      4        \u001b[36m0.8260\u001b[0m       0.4375        1.0046  0.0040\n",
      "      5        \u001b[36m0.8063\u001b[0m       0.5000        1.0242  0.0040\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0861\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m0.9795\u001b[0m  0.0036\n",
      "      2        \u001b[36m0.9649\u001b[0m       0.5625        \u001b[35m0.9411\u001b[0m  0.0038\n",
      "      3        \u001b[36m0.9037\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.9235\u001b[0m  0.0076\n",
      "      4        \u001b[36m0.8421\u001b[0m       0.5625        \u001b[35m0.9052\u001b[0m  0.0060\n",
      "      5        \u001b[36m0.7834\u001b[0m       0.5625        \u001b[35m0.9046\u001b[0m  0.0037\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0488\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m0.9977\u001b[0m  0.0034\n",
      "      2        \u001b[36m0.8934\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.9909\u001b[0m  0.0034\n",
      "      3        \u001b[36m0.8745\u001b[0m       0.5000        \u001b[35m0.9814\u001b[0m  0.0034\n",
      "      4        \u001b[36m0.8266\u001b[0m       0.5000        1.0224  0.0034\n",
      "      5        \u001b[36m0.8115\u001b[0m       0.4375        1.0394  0.0035\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1065\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.0979\u001b[0m  0.0034\n",
      "      2        \u001b[36m1.0794\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0793\u001b[0m  0.0034\n",
      "      3        \u001b[36m1.0581\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0612\u001b[0m  0.0033\n",
      "      4        \u001b[36m1.0370\u001b[0m       0.5000        \u001b[35m1.0431\u001b[0m  0.0034\n",
      "      5        \u001b[36m1.0142\u001b[0m       0.5000        \u001b[35m1.0249\u001b[0m  0.0033\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1079\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.0917\u001b[0m  0.0033\n",
      "      2        \u001b[36m1.0829\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.0727\u001b[0m  0.0033\n",
      "      3        \u001b[36m1.0609\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0546\u001b[0m  0.0033\n",
      "      4        \u001b[36m1.0390\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0363\u001b[0m  0.0034\n",
      "      5        \u001b[36m1.0157\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m1.0172\u001b[0m  0.0033\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1076\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.0955\u001b[0m  0.0032\n",
      "      2        \u001b[36m1.0808\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0785\u001b[0m  0.0032\n",
      "      3        \u001b[36m1.0575\u001b[0m       0.5625        \u001b[35m1.0633\u001b[0m  0.0032\n",
      "      4        \u001b[36m1.0341\u001b[0m       0.5625        \u001b[35m1.0492\u001b[0m  0.0033\n",
      "      5        \u001b[36m1.0102\u001b[0m       0.5000        \u001b[35m1.0362\u001b[0m  0.0039\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0863\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0107\u001b[0m  0.0043\n",
      "      2        \u001b[36m0.9734\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m0.9687\u001b[0m  0.0045\n",
      "      3        \u001b[36m0.9129\u001b[0m       0.5000        0.9810  0.0035\n",
      "      4        \u001b[36m0.8908\u001b[0m       0.5625        0.9808  0.0027\n",
      "      5        \u001b[36m0.8306\u001b[0m       0.4375        1.0086  0.0025\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0834\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0141\u001b[0m  0.0023\n",
      "      2        \u001b[36m0.9763\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.9464\u001b[0m  0.0024\n",
      "      3        \u001b[36m0.9042\u001b[0m       \u001b[32m0.7500\u001b[0m        \u001b[35m0.9135\u001b[0m  0.0041\n",
      "      4        \u001b[36m0.8529\u001b[0m       0.5625        \u001b[35m0.9060\u001b[0m  0.0039\n",
      "      5        \u001b[36m0.8117\u001b[0m       0.5625        \u001b[35m0.8980\u001b[0m  0.0038\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0917\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0358\u001b[0m  0.0038\n",
      "      2        \u001b[36m0.9714\u001b[0m       0.5000        \u001b[35m0.9992\u001b[0m  0.0039\n",
      "      3        \u001b[36m0.9123\u001b[0m       0.5000        \u001b[35m0.9870\u001b[0m  0.0039\n",
      "      4        \u001b[36m0.8639\u001b[0m       0.5000        1.0054  0.0039\n",
      "      5        \u001b[36m0.8368\u001b[0m       0.5000        1.0128  0.0035\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1099\u001b[0m       \u001b[32m0.3125\u001b[0m        \u001b[35m1.1054\u001b[0m  0.0029\n",
      "      2        \u001b[36m1.0929\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0939\u001b[0m  0.0024\n",
      "      3        \u001b[36m1.0785\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0825\u001b[0m  0.0023\n",
      "      4        \u001b[36m1.0644\u001b[0m       0.5000        \u001b[35m1.0714\u001b[0m  0.0024\n",
      "      5        \u001b[36m1.0517\u001b[0m       0.5625        \u001b[35m1.0602\u001b[0m  0.0038\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1126\u001b[0m       \u001b[32m0.3125\u001b[0m        \u001b[35m1.0998\u001b[0m  0.0038\n",
      "      2        \u001b[36m1.0961\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.0881\u001b[0m  0.0038\n",
      "      3        \u001b[36m1.0825\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0771\u001b[0m  0.0038\n",
      "      4        \u001b[36m1.0695\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.0666\u001b[0m  0.0038\n",
      "      5        \u001b[36m1.0570\u001b[0m       0.6250        \u001b[35m1.0559\u001b[0m  0.0039\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1115\u001b[0m       \u001b[32m0.3125\u001b[0m        \u001b[35m1.1011\u001b[0m  0.0028\n",
      "      2        \u001b[36m1.0960\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.0910\u001b[0m  0.0024\n",
      "      3        \u001b[36m1.0813\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0819\u001b[0m  0.0024\n",
      "      4        \u001b[36m1.0678\u001b[0m       0.5625        \u001b[35m1.0733\u001b[0m  0.0024\n",
      "      5        \u001b[36m1.0548\u001b[0m       0.5000        \u001b[35m1.0655\u001b[0m  0.0024\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1020\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0854\u001b[0m  0.0091\n",
      "      2        \u001b[36m1.0697\u001b[0m       0.5000        \u001b[35m1.0593\u001b[0m  0.0091\n",
      "      3        \u001b[36m1.0383\u001b[0m       0.4167        \u001b[35m1.0334\u001b[0m  0.0092\n",
      "      4        \u001b[36m1.0079\u001b[0m       0.5000        \u001b[35m1.0073\u001b[0m  0.0070\n",
      "      5        \u001b[36m0.9792\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m0.9857\u001b[0m  0.0055\n",
      "Best hyperparameters found: {'batch_size': 16, 'lr': 0.001}\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1010\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0840\u001b[0m  0.0057\n",
      "      2        \u001b[36m1.0673\u001b[0m       0.4583        \u001b[35m1.0557\u001b[0m  0.0089\n",
      "      3        \u001b[36m1.0356\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m1.0288\u001b[0m  0.0091\n",
      "      4        \u001b[36m1.0046\u001b[0m       0.5000        \u001b[35m1.0034\u001b[0m  0.0056\n",
      "      5        \u001b[36m0.9748\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.9814\u001b[0m  0.0055\n",
      "      6        \u001b[36m0.9516\u001b[0m       \u001b[32m0.7083\u001b[0m        \u001b[35m0.9643\u001b[0m  0.0055\n",
      "      7        \u001b[36m0.9318\u001b[0m       0.7083        \u001b[35m0.9510\u001b[0m  0.0054\n",
      "      8        \u001b[36m0.9167\u001b[0m       0.7083        \u001b[35m0.9408\u001b[0m  0.0092\n",
      "      9        \u001b[36m0.9053\u001b[0m       0.7083        \u001b[35m0.9349\u001b[0m  0.0090\n",
      "     10        \u001b[36m0.8948\u001b[0m       0.7083        \u001b[35m0.9284\u001b[0m  0.0056\n",
      "     11        \u001b[36m0.8918\u001b[0m       0.7083        \u001b[35m0.9248\u001b[0m  0.0055\n",
      "     12        \u001b[36m0.8801\u001b[0m       0.7083        \u001b[35m0.9200\u001b[0m  0.0091\n",
      "     13        \u001b[36m0.8712\u001b[0m       0.7083        \u001b[35m0.9167\u001b[0m  0.0091\n",
      "     14        \u001b[36m0.8623\u001b[0m       \u001b[32m0.7500\u001b[0m        \u001b[35m0.9156\u001b[0m  0.0092\n",
      "     15        \u001b[36m0.8549\u001b[0m       0.7500        0.9160  0.0092\n",
      "     16        \u001b[36m0.8472\u001b[0m       0.7500        0.9156  0.0092\n",
      "     17        \u001b[36m0.8418\u001b[0m       0.7083        \u001b[35m0.9156\u001b[0m  0.0092\n",
      "     18        \u001b[36m0.8330\u001b[0m       0.7500        0.9162  0.0079\n",
      "     19        \u001b[36m0.8270\u001b[0m       0.7500        \u001b[35m0.9154\u001b[0m  0.0059\n",
      "     20        \u001b[36m0.8180\u001b[0m       0.7500        0.9158  0.0053\n",
      "     21        \u001b[36m0.8105\u001b[0m       0.7500        0.9183  0.0090\n",
      "     22        \u001b[36m0.8035\u001b[0m       0.7500        0.9180  0.0091\n",
      "     23        \u001b[36m0.7965\u001b[0m       0.7500        0.9165  0.0092\n",
      "     24        \u001b[36m0.7872\u001b[0m       0.7500        0.9173  0.0092\n",
      "     25        \u001b[36m0.7826\u001b[0m       0.6667        0.9208  0.0094\n",
      "     26        \u001b[36m0.7749\u001b[0m       0.7083        0.9190  0.0093\n",
      "     27        \u001b[36m0.7650\u001b[0m       0.6667        0.9232  0.0093\n",
      "     28        \u001b[36m0.7559\u001b[0m       0.6667        0.9226  0.0093\n",
      "     29        \u001b[36m0.7489\u001b[0m       0.6250        0.9210  0.0106\n",
      "     30        \u001b[36m0.7419\u001b[0m       0.6250        0.9263  0.0064\n",
      "     31        \u001b[36m0.7360\u001b[0m       0.6250        0.9200  0.0092\n",
      "     32        \u001b[36m0.7277\u001b[0m       0.6250        0.9253  0.0092\n",
      "     33        \u001b[36m0.7188\u001b[0m       0.6250        0.9218  0.0080\n",
      "     34        \u001b[36m0.7130\u001b[0m       0.6250        0.9250  0.0092\n",
      "     35        \u001b[36m0.7052\u001b[0m       0.6250        0.9217  0.0093\n",
      "     36        \u001b[36m0.6985\u001b[0m       0.6250        0.9191  0.0092\n",
      "     37        \u001b[36m0.6907\u001b[0m       0.6250        0.9231  0.0085\n",
      "     38        \u001b[36m0.6848\u001b[0m       0.6250        0.9257  0.0064\n",
      "     39        \u001b[36m0.6794\u001b[0m       0.6250        0.9196  0.0053\n",
      "     40        \u001b[36m0.6730\u001b[0m       0.6250        0.9276  0.0053\n",
      "     41        \u001b[36m0.6660\u001b[0m       0.6250        0.9228  0.0052\n",
      "     42        \u001b[36m0.6610\u001b[0m       0.6250        0.9194  0.0053\n",
      "     43        \u001b[36m0.6557\u001b[0m       0.6250        0.9295  0.0053\n",
      "     44        \u001b[36m0.6503\u001b[0m       0.6250        0.9248  0.0054\n",
      "     45        \u001b[36m0.6442\u001b[0m       0.6250        0.9253  0.0050\n",
      "     46        \u001b[36m0.6408\u001b[0m       0.6250        0.9231  0.0047\n",
      "     47        \u001b[36m0.6350\u001b[0m       0.6250        0.9282  0.0048\n",
      "     48        \u001b[36m0.6311\u001b[0m       0.6250        0.9322  0.0047\n",
      "     49        \u001b[36m0.6273\u001b[0m       0.6250        0.9342  0.0048\n",
      "     50        \u001b[36m0.6237\u001b[0m       0.6250        0.9275  0.0047\n",
      "Test Accuracy: 63.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pennylane as qml\n",
    "\n",
    "# Define the quantum device and circuit\n",
    "n_qubits = 4  # Adjust based on input size\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_layer(inputs, weights):\n",
    "    # Apply encoding for inputs\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    # Add parameterized layers\n",
    "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Define the deep learning model for multi-class classification\n",
    "class DeepSNPNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeepSNPNet, self).__init__()\n",
    "        # Quantum weights\n",
    "        self.q_params = nn.Parameter(torch.randn(2, n_qubits))  # Adjustable based on circuit structure\n",
    "        self.fc1 = nn.Linear(n_qubits, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # 3 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert q_out to float32 explicitly\n",
    "        q_out = torch.tensor([quantum_layer(inputs=x[i], weights=self.q_params) for i in range(x.shape[0])], dtype=torch.float32) \n",
    "        x = torch.relu(self.fc1(q_out))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Set input size based on your data\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Initialize model for multi-class classification\n",
    "model = DeepSNPNet(input_size)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.001],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "\n",
    "# Wrap the model with skorch's NeuralNetClassifier for multi-class classification\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=5,\n",
    "    lr=0.01,\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Convert X_train to float32 before fitting\n",
    "X_train_float32 = X_train.astype(np.float32)\n",
    "\n",
    "# Convert training data into a DataLoader for better performance\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_float32), torch.tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(net, param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train_float32, y_train)\n",
    "\n",
    "# Retrieve and print the best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Apply best parameters to the model for final training\n",
    "optimized_net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=50,\n",
    "    lr=best_params['lr'],\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Convert test set to float32 for compatibility\n",
    "X_test_float32 = X_test.astype(np.float32)\n",
    "\n",
    "# Initialize scaler only if CUDA is available\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Train with the optimized model and parameters\n",
    "optimized_net.fit(X_train_float32, y_train)\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = optimized_net.predict(X_test_float32)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "AZ-Z9z20aHko"
   },
   "outputs": [],
   "source": [
    "device = next(optimized_net.module_.parameters()).device  # Get model's device\n",
    "dummy_input = torch.randn(1, X_train.shape[1], device=device)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(optimized_net.module_, dummy_input, \"snp_model.onnx\", opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQBFHfE7VFxl",
    "outputId": "5b26d360-5842-4e95-c3df-9a9840490ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted and saved to openvino_model/snp_model.xml\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "import os\n",
    "\n",
    "# Load the ONNX model\n",
    "core = ov.Core()\n",
    "model = core.read_model(\"snp_model.onnx\")\n",
    "\n",
    "# Specify input and output data types\n",
    "input_shape = ov.PartialShape([1, X_train.shape[1]])  # Input shape\n",
    "input_type = ov.Type.f32            # Input data type (FP32)\n",
    "output_type = ov.Type.f32           # Output data type (FP32)\n",
    "\n",
    "# Convert the model to OpenVINO IR with FP32 data type\n",
    "compiled_model = ov.compile_model(model, \"CPU\") # Compile for CPU\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"openvino_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the output file paths\n",
    "xml_path = os.path.join(output_dir, \"snp_model.xml\")\n",
    "bin_path = os.path.join(output_dir, \"snp_model.bin\")\n",
    "\n",
    "# Save the converted model\n",
    "ov.save_model(model, xml_path)  # Save the model\n",
    "print(f\"Model converted and saved to {xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJsxR5fyaRDP",
    "outputId": "c8bef613-942a-48f6-9f42-81595f654cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: {0: 34, 1: 34, 2: 37}\n",
      "Using SMOTE for balancing.\n",
      "Patient 1: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 1 probabilities: [0.12 0.48 0.4 ]\n",
      "Patient 2: Predicted Disease Risk - No Disease\n",
      "Patient 2 probabilities: [0.59 0.09 0.32]\n",
      "Patient 3: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 3 probabilities: [0.24 0.29 0.47]\n",
      "Patient 4: Predicted Disease Risk - No Disease\n",
      "Patient 4 probabilities: [0.43 0.32 0.25]\n",
      "Patient 5: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 5 probabilities: [0.28 0.37 0.35]\n",
      "Patient 6: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 6 probabilities: [0.27 0.24 0.49]\n",
      "Patient 7: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 7 probabilities: [0.34 0.23 0.43]\n",
      "Patient 8: Predicted Disease Risk - No Disease\n",
      "Patient 8 probabilities: [0.7  0.16 0.14]\n",
      "Patient 9: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 9 probabilities: [0.23 0.47 0.3 ]\n",
      "Patient 10: Predicted Disease Risk - No Disease\n",
      "Patient 10 probabilities: [0.6  0.31 0.09]\n",
      "Patient 11: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 11 probabilities: [0.25 0.5  0.25]\n",
      "Patient 12: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 12 probabilities: [0.3  0.35 0.35]\n",
      "Patient 13: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 13 probabilities: [0.26 0.51 0.23]\n",
      "Patient 14: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 14 probabilities: [0.27 0.41 0.32]\n",
      "Patient 15: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 15 probabilities: [0.16 0.42 0.42]\n",
      "Patient 16: Predicted Disease Risk - No Disease\n",
      "Patient 16 probabilities: [0.38 0.26 0.36]\n",
      "Patient 17: Predicted Disease Risk - No Disease\n",
      "Patient 17 probabilities: [0.46 0.32 0.22]\n",
      "Patient 18: Predicted Disease Risk - No Disease\n",
      "Patient 18 probabilities: [0.37 0.3  0.33]\n",
      "Patient 19: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 19 probabilities: [0.11 0.47 0.42]\n",
      "Patient 20: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 20 probabilities: [0.32 0.25 0.43]\n",
      "Patient 21: Predicted Disease Risk - No Disease\n",
      "Patient 21 probabilities: [0.55 0.34 0.11]\n",
      "Patient 22: Predicted Disease Risk - No Disease\n",
      "Patient 22 probabilities: [0.53 0.33 0.14]\n",
      "Patient 23: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 23 probabilities: [0.18 0.43 0.39]\n",
      "Patient 24: Predicted Disease Risk - No Disease\n",
      "Patient 24 probabilities: [0.62 0.24 0.14]\n",
      "Patient 25: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 25 probabilities: [0.36 0.46 0.18]\n",
      "Patient 26: Predicted Disease Risk - No Disease\n",
      "Patient 26 probabilities: [0.48 0.26 0.26]\n",
      "Patient 27: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 27 probabilities: [0.22 0.54 0.24]\n",
      "Patient 28: Predicted Disease Risk - No Disease\n",
      "Patient 28 probabilities: [0.42 0.29 0.29]\n",
      "Patient 29: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 29 probabilities: [0.16 0.5  0.34]\n",
      "Patient 30: Predicted Disease Risk - No Disease\n",
      "Patient 30 probabilities: [0.48 0.31 0.21]\n",
      "Patient 31: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 31 probabilities: [0.1  0.75 0.15]\n",
      "Patient 32: Predicted Disease Risk - No Disease\n",
      "Patient 32 probabilities: [0.44 0.26 0.3 ]\n",
      "Patient 33: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 33 probabilities: [0.27 0.32 0.41]\n",
      "Patient 34: Predicted Disease Risk - No Disease\n",
      "Patient 34 probabilities: [0.45 0.18 0.37]\n",
      "Patient 35: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 35 probabilities: [0.19 0.53 0.28]\n",
      "Patient 36: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 36 probabilities: [0.36 0.13 0.51]\n",
      "Patient 37: Predicted Disease Risk - No Disease\n",
      "Patient 37 probabilities: [0.55 0.22 0.23]\n",
      "Patient 38: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 38 probabilities: [0.23 0.44 0.33]\n",
      "Patient 39: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 39 probabilities: [0.35 0.36 0.29]\n",
      "Patient 40: Predicted Disease Risk - No Disease\n",
      "Patient 40 probabilities: [0.64 0.19 0.17]\n",
      "Patient 41: Predicted Disease Risk - No Disease\n",
      "Patient 41 probabilities: [0.39 0.25 0.36]\n",
      "Patient 42: Predicted Disease Risk - Possible Heart Disease\n",
      "Patient 42 probabilities: [0.18 0.44 0.38]\n",
      "Patient 43: Predicted Disease Risk - Possible Cancer Risk\n",
      "Patient 43 probabilities: [0.38 0.22 0.4 ]\n",
      "Patient 44: Predicted Disease Risk - No Disease\n",
      "Patient 44 probabilities: [0.43 0.4  0.17]\n",
      "Patient 45: Predicted Disease Risk - No Disease\n",
      "Patient 45 probabilities: [0.42 0.26 0.32]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Assuming SNP data is preprocessed and reduced via PCA\n",
    "# Generate synthetic disease labels for demonstration\n",
    "# 0 = 'No Disease', 1 = 'Heart Disease', 2 = 'Cancer Risk'\n",
    "disease_labels = np.random.choice([0, 1, 2], size=snp_array_reduced.shape[0])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(snp_array_reduced, disease_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check distribution of labels in training data\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Training label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Dynamically decide between SMOTE or RandomOverSampler based on class sizes\n",
    "if min(counts) < 2:\n",
    "    print(\"Using RandomOverSampler due to small class sizes.\")\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_res, y_train_res = oversampler.fit_resample(X_train, y_train)\n",
    "else:\n",
    "    print(\"Using SMOTE for balancing.\")\n",
    "    k_neighbors_value = min(counts) - 1  # To avoid the error, set neighbors based on smallest class size\n",
    "    sm = SMOTE(random_state=42, k_neighbors=k_neighbors_value)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Load the OpenVINO model\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=\"openvino_model/snp_model.xml\")\n",
    "compiled_model = ie.compile_model(model=model_ir, device_name=\"CPU\")  # Use CPU, GPU, etc.\n",
    "\n",
    "# Get input shape from OpenVINO model\n",
    "input_shape = compiled_model.input(0).shape  # Get the expected input shape\n",
    "num_features_openvino = input_shape[1]  # Number of features expected by OpenVINO\n",
    "\n",
    "# Prepare the input and output layers\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# RandomForest Classifier for better generalization\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    # ... other parameters (add more if needed)\n",
    "}\n",
    "# Create the VotingClassifier (ensemble_model)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(ensemble_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Run inference on the test data using OpenVINO and deep learning model\n",
    "input_data = X_test  # Ensure test data is in the correct format\n",
    "predictions_openvino = []\n",
    "predictions_deep_learning = []\n",
    "\n",
    "# Get the input size for the deep learning model\n",
    "deep_learning_input_size = optimized_net.module_.fc1.in_features  # Get input size from fc1 layer\n",
    "\n",
    "# Iterate over each test data point\n",
    "for i in range(X_test.shape[0]): # Use X_test directly for iteration\n",
    "    single_input = X_test[i].reshape(1, -1)  # Reshape to (1, num_features)\n",
    "\n",
    "    # Reshape input to match OpenVINO model's expected shape if necessary\n",
    "    if single_input.shape[1] != num_features_openvino:\n",
    "        # Pad or truncate the input to match the expected shape\n",
    "        single_input = np.pad(single_input, ((0, 0), (0, num_features_openvino - single_input.shape[1])), 'constant')\n",
    "\n",
    "    # OpenVINO inference\n",
    "    result_openvino = compiled_model([single_input])\n",
    "    predictions_openvino.append(result_openvino[output_layer])\n",
    "\n",
    "    # Reshape or pad/truncate the input to match deep learning model's input size\n",
    "    if single_input.shape[1] != deep_learning_input_size:\n",
    "        single_input_dl = np.pad(single_input, ((0, 0), (0, deep_learning_input_size - single_input.shape[1])), 'constant')\n",
    "    else:\n",
    "        single_input_dl = single_input\n",
    "\n",
    "    # Deep learning model inference\n",
    "    result_deep_learning = optimized_net.module_(torch.tensor(single_input_dl, dtype=torch.float32, device=device))  # Use optimized_net.module_\n",
    "    predictions_deep_learning.append(result_deep_learning.cpu().detach().numpy())\n",
    "\n",
    "# Stack predictions and convert to NumPy arrays\n",
    "predictions_openvino = np.vstack(predictions_openvino)\n",
    "predictions_deep_learning = np.vstack(predictions_deep_learning)\n",
    "\n",
    "# Combine predictions (e.g., averaging or weighted averaging)\n",
    "combined_predictions = (predictions_openvino + predictions_deep_learning) / 2  # Simple averaging\n",
    "\n",
    "# For multiclass, use argmax to get the predicted class index\n",
    "predicted_classes = np.argmax(combined_predictions, axis=1)\n",
    "\n",
    "# Map predicted classes to disease names\n",
    "disease_mapping = {0: 'No Disease', 1: 'Heart Disease', 2: 'Cancer Risk'}\n",
    "predicted_diseases = [disease_mapping.get(int(pred), 'Unknown') for pred in predicted_classes]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "\n",
    "# Confidence thresholding for better predictions\n",
    "def get_confident_predictions(output_value, confidence_threshold=0.7):\n",
    "    if np.max(output_value) > confidence_threshold:\n",
    "        return np.argmax(output_value)  # Confident prediction\n",
    "    else:\n",
    "        return -1  # Uncertain prediction\n",
    "\n",
    "# Adjust predictions based on confidence levels\n",
    "confident_predictions = [get_confident_predictions(result) for result in predictions]\n",
    "# Define the threshold for disease prediction\n",
    "disease_threshold = 0.6\n",
    "\n",
    "# Function to determine the disease risk based on raw output\n",
    "# This function needs to be adjusted to handle the output of the ensemble model\n",
    "def get_disease_risk(predicted_class):  # Changed input to predicted_class\n",
    "    # Map the predicted class to a risk level\n",
    "    if predicted_class == 0:\n",
    "        return \"No Disease\"\n",
    "    elif predicted_class == 1:\n",
    "        return \"Possible Heart Disease\"\n",
    "    elif predicted_class == 2:\n",
    "        return \"Possible Cancer Risk\"\n",
    "    else:\n",
    "        return \"Unknown\" # Handle unexpected class values\n",
    "\n",
    "\n",
    "# Accessing prediction probabilities for a more nuanced approach\n",
    "# This approach assumes the voting classifier can provide probabilities\n",
    "# Make sure you use voting='soft' in your VotingClassifier for this\n",
    "predictions_with_probs = ensemble_model.predict_proba(X_test)\n",
    "\n",
    "# Display the predicted disease name and probabilities for each patient\n",
    "for i, result in enumerate(predictions_with_probs):\n",
    "    # Get predicted class directly from result (using argmax for probabilities)\n",
    "    predicted_class = np.argmax(result)  # Get predicted class from probabilities\n",
    "    disease_risk = get_disease_risk(predicted_class)\n",
    "    print(f\"Patient {i+1}: Predicted Disease Risk - {disease_risk}\")\n",
    "    print(f\"Patient {i+1} probabilities: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGrL3b91LrdA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
