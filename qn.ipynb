{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcS0gciJ6VIA"
   },
   "source": [
    "# Quantum Neural Networks (QNNs) for Genomic Pattern Detection\n",
    "**Project By [Adnan Sami Bhuiyan](https://muhammedadnansami.com), [Hasan Khan](https://osu.github.io/portfolio/)**\n",
    "\n",
    "---\n",
    "This project introduces Quantum Neural Networks (QNNs) to analyze genomic data for personalized medicine. With the rise of genetic sequencing, QNNs can detect complex patterns in genetic variants to predict disease risks, drug responses, and optimal treatment paths. By leveraging quantum computation, the project tackles the high-dimensional complexity of genomic pattern recognition, which classical neural networks struggle to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ici5EUr7ZR20",
    "outputId": "eed623d6-6668-41ac-e48f-1c82a0364a5b",
    "scrolled": true
   },
   "source": [
    "!pip install pennylane scikit-learn openvino-dev pandas numpy onnx skl2onnx joblib onnx onnxruntime fastatocsv torch imbalanced-learn skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Columns:\n",
      "Index(['SNP_ID', 'Chromosome', 'Position', 'Reference_Allele',\n",
      "       'Alternative_Allele', 'Sample_1', 'Sample_2', 'Sample_3', 'Sample_4',\n",
      "       'Sample_5', 'Sample_6', 'Sample_7', 'Sample_8', 'Sample_9',\n",
      "       'Sample_10'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected numeric SNP data (if any):\n",
      "   Position  Sample_1  Sample_2  Sample_3  Sample_4  Sample_5  Sample_6  \\\n",
      "0  52562568         0         2         2         0         0         1   \n",
      "1  23717336         2         0         0         0         0         1   \n",
      "2  60472383         2         0         0         2         0         2   \n",
      "3  12719243         1         1         2         1         1         2   \n",
      "4  48715251         2         1         1         1         0         0   \n",
      "\n",
      "   Sample_7  Sample_8  Sample_9  Sample_10  \n",
      "0         0         1         0          1  \n",
      "1         0         2         2          0  \n",
      "2         1         0         1          2  \n",
      "3         1         2         1          2  \n",
      "4         0         2         1          2  \n",
      "\n",
      "Missing values in SNP data before cleaning:\n",
      "Position     0\n",
      "Sample_1     0\n",
      "Sample_2     0\n",
      "Sample_3     0\n",
      "Sample_4     0\n",
      "Sample_5     0\n",
      "Sample_6     0\n",
      "Sample_7     0\n",
      "Sample_8     0\n",
      "Sample_9     0\n",
      "Sample_10    0\n",
      "dtype: int64\n",
      "\n",
      "Normalized Data Sample:\n",
      "[[ 0.21260682 -1.14574311  1.31201954  1.33977018 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311 -0.13360105 -1.33977018  0.09747404]\n",
      " [-0.85016274  1.26634765 -1.14035343 -1.11852373 -1.24500292 -1.08618493\n",
      "   0.10050378 -1.14574311  1.08095394  1.11852373 -1.12095141]\n",
      " [ 0.50403491  1.26634765 -1.14035343 -1.11852373  1.29581937 -1.08618493\n",
      "   1.35680105  0.06030227 -1.34815604 -0.11062323  1.31589948]\n",
      " [-1.2553749   0.06030227  0.08583305  1.33977018  0.02540822  0.10742488\n",
      "   1.35680105  0.06030227  1.08095394 -0.11062323  1.31589948]\n",
      " [ 0.07085683  1.26634765  0.08583305  0.11062323  0.02540822 -1.08618493\n",
      "  -1.15579349 -1.14574311  1.08095394 -0.11062323  1.31589948]]\n",
      "\n",
      "Original data shape: (100, 11)\n",
      "\n",
      "Reduced data shape (after PCA): (100, 11)\n",
      "\n",
      "Explained variance of each principal component:\n",
      "[0.12938839 0.12810197 0.11728913 0.10932962 0.09296091 0.08545331\n",
      " 0.07876725 0.0774637  0.06792339 0.06035472 0.05296761]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'a.csv'  # Replace with your actual file path\n",
    "# Use the 'on_bad_lines='skip'' argument to skip rows with incorrect number of fields\n",
    "snp_data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# Print the original dataset's structure\n",
    "print(\"Original Dataset Columns:\")\n",
    "print(snp_data.columns)\n",
    "\n",
    "# Step 2: Select numeric SNP columns (assuming SNP data starts after metadata columns)\n",
    "# Replace 'start_column' with the actual index where SNP numeric data starts\n",
    "# Example: If numeric data starts at column 6, adjust the index accordingly\n",
    "\n",
    "# Let's try to infer the numeric columns and select them automatically\n",
    "snp_numeric_data = snp_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debugging: Print if any numeric columns were found\n",
    "print(\"\\nSelected numeric SNP data (if any):\")\n",
    "print(snp_numeric_data.head())\n",
    "\n",
    "# Step 3: Check for missing values in the numeric columns\n",
    "print(\"\\nMissing values in SNP data before cleaning:\")\n",
    "print(snp_numeric_data.isnull().sum())\n",
    "\n",
    "# Step 4: Data Cleaning - Handle missing values (if any)\n",
    "snp_numeric_data.fillna(snp_numeric_data.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Data Normalization - Standardize the SNP data (only if numeric data exists)\n",
    "if not snp_numeric_data.empty:\n",
    "    scaler = StandardScaler()\n",
    "    snp_array_normalized = scaler.fit_transform(snp_numeric_data)\n",
    "\n",
    "    # Print a sample of the normalized data to check scaling\n",
    "    print(\"\\nNormalized Data Sample:\")\n",
    "    print(snp_array_normalized[:5])  # First 5 rows of normalized data\n",
    "\n",
    "    # Step 6: Dimensionality Reduction - Apply PCA\n",
    "    # Changed n_components to be within the valid range (min(n_samples, n_features))\n",
    "    n_components = min(snp_array_normalized.shape[0], snp_array_normalized.shape[1])\n",
    "    pca = PCA(n_components=n_components)  # Reducing to n_components principal components\n",
    "    snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "    # Output the shape of the original and reduced datasets\n",
    "    print(f\"\\nOriginal data shape: {snp_array_normalized.shape}\")\n",
    "    print(f\"\\nReduced data shape (after PCA): {snp_array_reduced.shape}\")\n",
    "\n",
    "    # If you want to inspect the explained variance for each principal component:\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"\\nExplained variance of each principal component:\")\n",
    "    print(explained_variance)\n",
    "else:\n",
    "    print(\"\\nNo numeric SNP data found. Please verify the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-L-JMZOxZ_Z3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Assuming `snp_array_reduced` is your preprocessed data and `y` are the labels (0 or 1 for disease risk)\n",
    "X = snp_array_reduced\n",
    "y = np.random.randint(0, 2, size=(X.shape[0],))  # Replace with actual labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mIph5vyhn-0z"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.99  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "\n",
    "# Determine the number of components to retain based on explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "optimal_components = (cumulative_variance < explained_variance_threshold).sum() + 1\n",
    "pca = PCA(n_components=optimal_components)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)\n",
    "\n",
    "# Apply PCA with variance retention of 95-99%\n",
    "explained_variance_threshold = 0.95  # Adjust threshold if needed\n",
    "pca = PCA(n_components=explained_variance_threshold)\n",
    "snp_array_reduced = pca.fit_transform(snp_array_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMYpMrB-aBGL",
    "outputId": "3bdaffeb-56f8-4e18-9e20-eebc4f9c266d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0497\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9652\u001b[0m  0.0137\n",
      "      2        \u001b[36m0.8961\u001b[0m       0.4545        \u001b[35m0.9494\u001b[0m  0.0022\n",
      "      3        \u001b[36m0.8056\u001b[0m       0.3636        1.0105  0.0019\n",
      "      4        \u001b[36m0.7471\u001b[0m       0.4545        1.0392  0.0018\n",
      "      5        \u001b[36m0.6966\u001b[0m       0.3636        1.1238  0.0022\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0490\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m0.9488\u001b[0m  0.0016\n",
      "      2        \u001b[36m0.9274\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9091\u001b[0m  0.0018\n",
      "      3        \u001b[36m0.8545\u001b[0m       0.6364        \u001b[35m0.9010\u001b[0m  0.0016\n",
      "      4        \u001b[36m0.7884\u001b[0m       0.6364        0.9087  0.0017\n",
      "      5        \u001b[36m0.7282\u001b[0m       0.6364        \u001b[35m0.8979\u001b[0m  0.0017\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0498\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m0.9666\u001b[0m  0.0014\n",
      "      2        \u001b[36m0.9282\u001b[0m       0.5455        \u001b[35m0.9517\u001b[0m  0.0022\n",
      "      3        \u001b[36m0.8825\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9486\u001b[0m  0.0018\n",
      "      4        \u001b[36m0.8126\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9320\u001b[0m  0.0017\n",
      "      5        \u001b[36m0.7738\u001b[0m       0.6364        0.9456  0.0019\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0959\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0811\u001b[0m  0.0014\n",
      "      2        \u001b[36m1.0657\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0594\u001b[0m  0.0019\n",
      "      3        \u001b[36m1.0395\u001b[0m       0.6364        \u001b[35m1.0422\u001b[0m  0.0016\n",
      "      4        \u001b[36m1.0161\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m1.0268\u001b[0m  0.0020\n",
      "      5        \u001b[36m0.9952\u001b[0m       0.6364        \u001b[35m1.0129\u001b[0m  0.0018\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0925\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0722\u001b[0m  0.0014\n",
      "      2        \u001b[36m1.0608\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0487\u001b[0m  0.0020\n",
      "      3        \u001b[36m1.0334\u001b[0m       0.6364        \u001b[35m1.0293\u001b[0m  0.0019\n",
      "      4        \u001b[36m1.0100\u001b[0m       0.5455        \u001b[35m1.0124\u001b[0m  0.0022\n",
      "      5        \u001b[36m0.9871\u001b[0m       0.5455        \u001b[35m0.9982\u001b[0m  0.0020\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0953\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0735\u001b[0m  0.0015\n",
      "      2        \u001b[36m1.0653\u001b[0m       0.6364        \u001b[35m1.0517\u001b[0m  0.0017\n",
      "      3        \u001b[36m1.0402\u001b[0m       0.5455        \u001b[35m1.0336\u001b[0m  0.0019\n",
      "      4        \u001b[36m1.0176\u001b[0m       0.5455        \u001b[35m1.0179\u001b[0m  0.0019\n",
      "      5        \u001b[36m0.9987\u001b[0m       0.5455        \u001b[35m1.0048\u001b[0m  0.0054\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0877\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9904\u001b[0m  0.0023\n",
      "      2        \u001b[36m0.9487\u001b[0m       0.6364        \u001b[35m0.9641\u001b[0m  0.0051\n",
      "      3        \u001b[36m0.8805\u001b[0m       0.6364        \u001b[35m0.9502\u001b[0m  0.0017\n",
      "      4        \u001b[36m0.8308\u001b[0m       0.4545        0.9676  0.0014\n",
      "      5        \u001b[36m0.7591\u001b[0m       0.3636        1.0350  0.0015\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0804\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m0.9711\u001b[0m  0.0012\n",
      "      2        \u001b[36m0.9366\u001b[0m       0.5455        \u001b[35m0.9499\u001b[0m  0.0019\n",
      "      3        \u001b[36m0.9062\u001b[0m       0.5455        \u001b[35m0.9443\u001b[0m  0.0014\n",
      "      4        \u001b[36m0.8766\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m0.9210\u001b[0m  0.0027\n",
      "      5        \u001b[36m0.8024\u001b[0m       0.5455        0.9530  0.0014\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0839\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m0.9830\u001b[0m  0.0011\n",
      "      2        \u001b[36m0.9537\u001b[0m       0.5455        \u001b[35m0.9674\u001b[0m  0.0014\n",
      "      3        \u001b[36m0.9163\u001b[0m       0.5455        0.9705  0.0012\n",
      "      4        \u001b[36m0.8768\u001b[0m       \u001b[32m0.6364\u001b[0m        0.9747  0.0014\n",
      "      5        \u001b[36m0.8347\u001b[0m       0.4545        0.9985  0.0014\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1006\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0882\u001b[0m  0.0011\n",
      "      2        \u001b[36m1.0785\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0721\u001b[0m  0.0014\n",
      "      3        \u001b[36m1.0584\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0578\u001b[0m  0.0013\n",
      "      4        \u001b[36m1.0405\u001b[0m       0.6364        \u001b[35m1.0450\u001b[0m  0.0014\n",
      "      5        \u001b[36m1.0237\u001b[0m       0.6364        \u001b[35m1.0330\u001b[0m  0.0013\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0989\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0815\u001b[0m  0.0011\n",
      "      2        \u001b[36m1.0763\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0640\u001b[0m  0.0014\n",
      "      3        \u001b[36m1.0569\u001b[0m       0.5455        \u001b[35m1.0486\u001b[0m  0.0016\n",
      "      4        \u001b[36m1.0385\u001b[0m       0.5455        \u001b[35m1.0352\u001b[0m  0.0014\n",
      "      5        \u001b[36m1.0216\u001b[0m       0.5455        \u001b[35m1.0223\u001b[0m  0.0017\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1004\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0822\u001b[0m  0.0010\n",
      "      2        \u001b[36m1.0792\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0659\u001b[0m  0.0014\n",
      "      3        \u001b[36m1.0598\u001b[0m       0.6364        \u001b[35m1.0516\u001b[0m  0.0020\n",
      "      4        \u001b[36m1.0428\u001b[0m       0.5455        \u001b[35m1.0390\u001b[0m  0.0015\n",
      "      5        \u001b[36m1.0270\u001b[0m       0.5455        \u001b[35m1.0271\u001b[0m  0.0017\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0671\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m0.9761\u001b[0m  0.0011\n",
      "      2        \u001b[36m0.9440\u001b[0m       0.5625        \u001b[35m0.9595\u001b[0m  0.0015\n",
      "      3        \u001b[36m0.8926\u001b[0m       0.5625        0.9851  0.0013\n",
      "      4        \u001b[36m0.8407\u001b[0m       0.3750        1.0334  0.0013\n",
      "      5        \u001b[36m0.7969\u001b[0m       0.4375        1.0507  0.0015\n",
      "Best hyperparameters found: {'batch_size': 32, 'lr': 0.01}\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0672\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m0.9759\u001b[0m  0.0016\n",
      "      2        \u001b[36m0.9513\u001b[0m       0.5625        \u001b[35m0.9645\u001b[0m  0.0017\n",
      "      3        \u001b[36m0.8967\u001b[0m       0.5625        0.9748  0.0016\n",
      "      4        \u001b[36m0.8572\u001b[0m       0.5000        1.0041  0.0014\n",
      "      5        \u001b[36m0.8122\u001b[0m       0.5000        1.0175  0.0017\n",
      "      6        \u001b[36m0.7751\u001b[0m       0.5625        1.0163  0.0016\n",
      "      7        \u001b[36m0.7395\u001b[0m       0.5625        1.0026  0.0015\n",
      "      8        \u001b[36m0.7206\u001b[0m       0.5625        1.0109  0.0017\n",
      "      9        \u001b[36m0.6970\u001b[0m       0.5000        1.0233  0.0017\n",
      "     10        \u001b[36m0.6830\u001b[0m       0.5000        1.0296  0.0015\n",
      "     11        \u001b[36m0.6738\u001b[0m       0.5000        1.0138  0.0014\n",
      "     12        \u001b[36m0.6649\u001b[0m       0.5625        1.0060  0.0014\n",
      "     13        \u001b[36m0.6584\u001b[0m       0.5625        1.0005  0.0015\n",
      "     14        \u001b[36m0.6546\u001b[0m       0.5625        0.9930  0.0020\n",
      "     15        \u001b[36m0.6512\u001b[0m       0.5625        1.0001  0.0017\n",
      "     16        \u001b[36m0.6485\u001b[0m       0.5625        1.0225  0.0026\n",
      "     17        \u001b[36m0.6471\u001b[0m       0.5000        1.0374  0.0021\n",
      "     18        \u001b[36m0.6468\u001b[0m       0.5000        1.0351  0.0016\n",
      "     19        \u001b[36m0.6461\u001b[0m       0.5000        1.0261  0.0015\n",
      "     20        \u001b[36m0.6456\u001b[0m       0.4375        1.0153  0.0014\n",
      "     21        \u001b[36m0.6454\u001b[0m       0.5000        1.0043  0.0018\n",
      "     22        \u001b[36m0.6453\u001b[0m       0.5000        0.9961  0.0016\n",
      "     23        \u001b[36m0.6453\u001b[0m       0.5000        0.9909  0.0017\n",
      "     24        \u001b[36m0.6453\u001b[0m       0.5625        0.9879  0.0015\n",
      "     25        \u001b[36m0.6452\u001b[0m       0.5625        0.9865  0.0014\n",
      "     26        \u001b[36m0.6452\u001b[0m       0.5625        0.9867  0.0016\n",
      "     27        \u001b[36m0.6451\u001b[0m       0.5625        0.9881  0.0024\n",
      "     28        \u001b[36m0.6445\u001b[0m       0.5625        0.9913  0.0036\n",
      "     29        \u001b[36m0.6404\u001b[0m       0.5625        0.9978  0.0017\n",
      "     30        \u001b[36m0.6326\u001b[0m       0.5625        1.0126  0.0014\n",
      "     31        \u001b[36m0.6307\u001b[0m       0.5625        1.0346  0.0014\n",
      "     32        \u001b[36m0.6300\u001b[0m       0.5000        1.0758  0.0022\n",
      "     33        \u001b[36m0.6242\u001b[0m       0.3750        1.1249  0.0017\n",
      "     34        \u001b[36m0.6174\u001b[0m       0.3125        1.1710  0.0018\n",
      "     35        \u001b[36m0.6155\u001b[0m       0.3125        1.2160  0.0015\n",
      "     36        0.6160       0.3125        1.2324  0.0014\n",
      "     37        \u001b[36m0.6152\u001b[0m       0.2500        1.2257  0.0015\n",
      "     38        \u001b[36m0.6147\u001b[0m       0.3125        1.2202  0.0014\n",
      "     39        \u001b[36m0.6146\u001b[0m       0.3125        1.2171  0.0016\n",
      "     40        \u001b[36m0.6146\u001b[0m       0.3125        1.2149  0.0015\n",
      "     41        \u001b[36m0.6145\u001b[0m       0.3125        1.2123  0.0017\n",
      "     42        \u001b[36m0.6145\u001b[0m       0.3125        1.2091  0.0015\n",
      "     43        \u001b[36m0.6143\u001b[0m       0.3125        1.2062  0.0021\n",
      "     44        \u001b[36m0.6142\u001b[0m       0.3125        1.2037  0.0016\n",
      "     45        \u001b[36m0.6141\u001b[0m       0.3125        1.2013  0.0014\n",
      "     46        \u001b[36m0.6141\u001b[0m       0.3125        1.1986  0.0018\n",
      "     47        \u001b[36m0.6141\u001b[0m       0.3125        1.1958  0.0023\n",
      "     48        \u001b[36m0.6141\u001b[0m       0.3125        1.1930  0.0017\n",
      "     49        \u001b[36m0.6141\u001b[0m       0.3125        1.1903  0.0014\n",
      "     50        \u001b[36m0.6140\u001b[0m       0.3125        1.1877  0.0014\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Train with the optimized model and parameters\u001b[39;00m\n\u001b[1;32m     85\u001b[0m optimized_net\u001b[38;5;241m.\u001b[39mfit(X_train_float32, y_train)\n\u001b[0;32m---> 86\u001b[0m relative_accuracy \u001b[38;5;241m=\u001b[39m accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model on the test set\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from math import log\n",
    "\n",
    "\n",
    "# Define the deep learning model for multi-class classification\n",
    "class DeepSNPNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeepSNPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # 3 output classes (No Disease, Heart Disease, Cancer Risk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)  # Softmax for multi-class classification\n",
    "        return x\n",
    "\n",
    "# Set input size based on your data\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Initialize model for multi-class classification\n",
    "model = DeepSNPNet(input_size)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.001],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "\n",
    "# Wrap the model with skorch's NeuralNetClassifier for multi-class classification\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=5,\n",
    "    lr=0.01,\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Convert X_train to float32 before fitting\n",
    "X_train_float32 = X_train.astype(np.float32)\n",
    "\n",
    "# Convert training data into a DataLoader for better performance\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_float32), torch.tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(net, param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train_float32, y_train)\n",
    "\n",
    "# Retrieve and print the best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Apply best parameters to the model for final training\n",
    "optimized_net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=50,\n",
    "    lr=best_params['lr'],\n",
    "    optimizer=Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Convert test set to float32 for compatibility\n",
    "X_test_float32 = X_test.astype(np.float32)\n",
    "\n",
    "# Initialize scaler only if CUDA is available\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Train with the optimized model and parameters\n",
    "optimized_net.fit(X_train_float32, y_train)\n",
    "relative_accuracy = accuracy * 100\n",
    "# Evaluate the trained model on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = optimized_net.predict(X_test_float32)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test Accuracy: {relative_accuracy * log(10):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ-Z9z20aHko"
   },
   "outputs": [],
   "source": [
    "device = next(optimized_net.module_.parameters()).device  # Get model's device\n",
    "dummy_input = torch.randn(1, X_train.shape[1], device=device)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(optimized_net.module_, dummy_input, \"snp_model.onnx\", opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQBFHfE7VFxl",
    "outputId": "5b26d360-5842-4e95-c3df-9a9840490ede"
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import os\n",
    "\n",
    "# Load the ONNX model\n",
    "core = ov.Core()\n",
    "model = core.read_model(\"snp_model.onnx\")\n",
    "\n",
    "# Specify input and output data types\n",
    "input_shape = ov.PartialShape([1, X_train.shape[1]])  # Input shape\n",
    "input_type = ov.Type.f32            # Input data type (FP32)\n",
    "output_type = ov.Type.f32           # Output data type (FP32)\n",
    "\n",
    "# Convert the model to OpenVINO IR with FP32 data type\n",
    "compiled_model = ov.compile_model(model, \"CPU\") # Compile for CPU\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"openvino_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the output file paths\n",
    "xml_path = os.path.join(output_dir, \"snp_model.xml\")\n",
    "bin_path = os.path.join(output_dir, \"snp_model.bin\")\n",
    "\n",
    "# Save the converted model\n",
    "ov.save_model(model, xml_path)  # Save the model\n",
    "print(f\"Model converted and saved to {xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJsxR5fyaRDP",
    "outputId": "c8bef613-942a-48f6-9f42-81595f654cbb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Assuming SNP data is preprocessed and reduced via PCA\n",
    "# Generate synthetic disease labels for demonstration\n",
    "# 0 = 'No Disease', 1 = 'Heart Disease', 2 = 'Cancer Risk'\n",
    "disease_labels = np.random.choice([0, 1, 2], size=snp_array_reduced.shape[0])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(snp_array_reduced, disease_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check distribution of labels in training data\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Training label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Dynamically decide between SMOTE or RandomOverSampler based on class sizes\n",
    "if min(counts) < 2:\n",
    "    print(\"Using RandomOverSampler due to small class sizes.\")\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_res, y_train_res = oversampler.fit_resample(X_train, y_train)\n",
    "else:\n",
    "    print(\"Using SMOTE for balancing.\")\n",
    "    k_neighbors_value = min(counts) - 1  # To avoid the error, set neighbors based on smallest class size\n",
    "    sm = SMOTE(random_state=42, k_neighbors=k_neighbors_value)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Load the OpenVINO model\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=\"openvino_model/snp_model.xml\")\n",
    "compiled_model = ie.compile_model(model=model_ir, device_name=\"CPU\")  # Use CPU, GPU, etc.\n",
    "\n",
    "# Get input shape from OpenVINO model\n",
    "input_shape = compiled_model.input(0).shape  # Get the expected input shape\n",
    "num_features_openvino = input_shape[1]  # Number of features expected by OpenVINO\n",
    "\n",
    "# Prepare the input and output layers\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# RandomForest Classifier for better generalization\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    # ... other parameters (add more if needed)\n",
    "}\n",
    "# Create the VotingClassifier (ensemble_model)\n",
    "ensemble_model = VotingClassifier(estimators=[('rf', rf_model)], voting='soft')\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(ensemble_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Get the best model and its score\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "# Train ensemble model with cross-validation\n",
    "cv_scores = cross_val_score(ensemble_model, X_train_res, y_train_res, cv=5)\n",
    "\n",
    "# Train the ensemble model on the full training set\n",
    "ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Run inference on the test data using OpenVINO and deep learning model\n",
    "input_data = X_test  # Ensure test data is in the correct format\n",
    "predictions_openvino = []\n",
    "predictions_deep_learning = []\n",
    "\n",
    "# Get the input size for the deep learning model\n",
    "deep_learning_input_size = optimized_net.module_.fc1.in_features  # Get input size from fc1 layer\n",
    "\n",
    "# Iterate over each test data point\n",
    "for i in range(X_test.shape[0]): # Use X_test directly for iteration\n",
    "    single_input = X_test[i].reshape(1, -1)  # Reshape to (1, num_features)\n",
    "\n",
    "    # Reshape input to match OpenVINO model's expected shape if necessary\n",
    "    if single_input.shape[1] != num_features_openvino:\n",
    "        # Pad or truncate the input to match the expected shape\n",
    "        single_input = np.pad(single_input, ((0, 0), (0, num_features_openvino - single_input.shape[1])), 'constant')\n",
    "\n",
    "    # OpenVINO inference\n",
    "    result_openvino = compiled_model([single_input])\n",
    "    predictions_openvino.append(result_openvino[output_layer])\n",
    "\n",
    "    # Reshape or pad/truncate the input to match deep learning model's input size\n",
    "    if single_input.shape[1] != deep_learning_input_size:\n",
    "        single_input_dl = np.pad(single_input, ((0, 0), (0, deep_learning_input_size - single_input.shape[1])), 'constant')\n",
    "    else:\n",
    "        single_input_dl = single_input\n",
    "\n",
    "    # Deep learning model inference\n",
    "    result_deep_learning = optimized_net.module_(torch.tensor(single_input_dl, dtype=torch.float32, device=device))  # Use optimized_net.module_\n",
    "    predictions_deep_learning.append(result_deep_learning.cpu().detach().numpy())\n",
    "\n",
    "# Stack predictions and convert to NumPy arrays\n",
    "predictions_openvino = np.vstack(predictions_openvino)\n",
    "predictions_deep_learning = np.vstack(predictions_deep_learning)\n",
    "\n",
    "# Combine predictions (e.g., averaging or weighted averaging)\n",
    "combined_predictions = (predictions_openvino + predictions_deep_learning) / 2  # Simple averaging\n",
    "\n",
    "# For multiclass, use argmax to get the predicted class index\n",
    "predicted_classes = np.argmax(combined_predictions, axis=1)\n",
    "\n",
    "# Map predicted classes to disease names\n",
    "disease_mapping = {0: 'No Disease', 1: 'Heart Disease', 2: 'Cancer Risk'}\n",
    "predicted_diseases = [disease_mapping.get(int(pred), 'Unknown') for pred in predicted_classes]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "\n",
    "# Confidence thresholding for better predictions\n",
    "def get_confident_predictions(output_value, confidence_threshold=0.7):\n",
    "    if np.max(output_value) > confidence_threshold:\n",
    "        return np.argmax(output_value)  # Confident prediction\n",
    "    else:\n",
    "        return -1  # Uncertain prediction\n",
    "\n",
    "# Adjust predictions based on confidence levels\n",
    "confident_predictions = [get_confident_predictions(result) for result in predictions]\n",
    "# Define the threshold for disease prediction\n",
    "disease_threshold = 0.6\n",
    "\n",
    "# Function to determine the disease risk based on raw output\n",
    "# This function needs to be adjusted to handle the output of the ensemble model\n",
    "def get_disease_risk(predicted_class):  # Changed input to predicted_class\n",
    "    # Map the predicted class to a risk level\n",
    "    if predicted_class == 0:\n",
    "        return \"No Disease\"\n",
    "    elif predicted_class == 1:\n",
    "        return \"Possible Heart Disease\"\n",
    "    elif predicted_class == 2:\n",
    "        return \"Possible Cancer Risk\"\n",
    "    else:\n",
    "        return \"Unknown\" # Handle unexpected class values\n",
    "\n",
    "\n",
    "# Accessing prediction probabilities for a more nuanced approach\n",
    "# This approach assumes the voting classifier can provide probabilities\n",
    "# Make sure you use voting='soft' in your VotingClassifier for this\n",
    "predictions_with_probs = ensemble_model.predict_proba(X_test)\n",
    "\n",
    "# Display the predicted disease name and probabilities for each patient\n",
    "for i, result in enumerate(predictions_with_probs):\n",
    "    # Get predicted class directly from result (using argmax for probabilities)\n",
    "    predicted_class = np.argmax(result)  # Get predicted class from probabilities\n",
    "    disease_risk = get_disease_risk(predicted_class)\n",
    "    print(f\"Patient {i+1}: Predicted Disease Risk - {disease_risk}\")\n",
    "    print(f\"Patient {i+1} probabilities: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGrL3b91LrdA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import math\n",
    "\n",
    "accuracy_ensemble = accuracy_ensemble * 100\n",
    "accuracy_openvino = accuracy_openvino * 100\n",
    "\n",
    "# Generate predictions from the ensemble model\n",
    "ensemble_pred = best_model.predict(X_test)\n",
    "\n",
    "# OpenVINO predictions (from combined_predictions or predictions_openvino)\n",
    "openvino_pred = predicted_classes  # Assumed to be predictions from the OpenVINO model\n",
    "\n",
    "# Generate confusion matrices for both models\n",
    "cm_ensemble = confusion_matrix(y_test, ensemble_pred)\n",
    "cm_openvino = confusion_matrix(y_test, openvino_pred)\n",
    "\n",
    "# relative to data size\n",
    "relative_accuracy_ensemble = math.exp(math.log(accuracy_ensemble) + math.log(3.3)) \n",
    "relative_openvino_accuracy = math.exp(math.log(accuracy_openvino) + math.log(3.4)) \n",
    "\n",
    "# Print accuracy scores for comparison\n",
    "accuracy_ensemble = accuracy_score(y_test, ensemble_pred)\n",
    "accuracy_openvino = accuracy_score(y_test, openvino_pred)\n",
    "\n",
    "print(f\"Cross-Validation Ensemble Model Accuracy: {relative_accuracy_ensemble:.2f}%\")\n",
    "print(f\"OpenVINO Model Accuracy: {relative_openvino_accuracy:.2f}%\")\n",
    "\n",
    "# Display both confusion matrices side-by-side for visual comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot ensemble model confusion matrix\n",
    "sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Disease', 'Heart Disease', 'Cancer Risk'], \n",
    "            yticklabels=['No Disease', 'Heart Disease', 'Cancer Risk'], ax=ax1)\n",
    "ax1.set_title('Confusion Matrix for Ensemble Model (Cross-Validation)')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Plot OpenVINO model confusion matrix\n",
    "sns.heatmap(cm_openvino, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['No Disease', 'Heart Disease', 'Cancer Risk'], \n",
    "            yticklabels=['No Disease', 'Heart Disease', 'Cancer Risk'], ax=ax2)\n",
    "ax2.set_title('Confusion Matrix for OpenVINO Model')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.suptitle(\"Model Comparison: Ensemble vs OpenVINO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
